# titan_backend.py
"""
TITAN Backend - Single-file Streamlit app
- No front-page login: this is an admin backend (Admin access by default)
- User & Role management available in Settings
- SQLite persistence via SQLAlchemy
- Internal ML training & scoring (no user tuning)
- Pipeline dashboard, Analytics, CPA/ROI, Exports/Imports, Alerts, SLA, Priority scoring, Audit trail
"""



import os
from datetime import datetime, timedelta, date
import io, base64, traceback
import streamlit as st
import folium
from streamlit_folium import st_folium
import pandas as pd
import numpy as np
import plotly.express as px
from datetime import datetime
import streamlit as st
import joblib

import secrets
import uuid

import smtplib
from email.message import EmailMessage

import re

import jwt
from datetime import datetime, timedelta

from sqlalchemy import (
    create_engine,
    Column,
    Integer,
    String,
    Float,
    Boolean,
    DateTime,
    Text,
    ForeignKey,
    inspect,
    text
)

from sqlalchemy.orm import declarative_base, sessionmaker, relationship
from sqlalchemy.exc import OperationalError, SQLAlchemyError
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

from sqlalchemy import inspect

import uuid
from sqlalchemy import ForeignKey
from sqlalchemy.orm import relationship






# ---------- Country list helper (robust) ----------
import requests
from functools import lru_cache

@lru_cache(maxsize=1)
def get_all_countries():
    """
    Robust fetch of all countries. Uses restcountries.com.
    Returns list of dicts: [{ 'name': 'United States', 'code': 'US' }, ...]
    Falls back to a small built-in list if the request fails.
    """
    FALLBACK = [
        {"name":"United States","code":"US"},
        {"name":"Canada","code":"CA"},
        {"name":"United Kingdom","code":"GB"},
        {"name":"Australia","code":"AU"}
    ]
    url = "https://restcountries.com/v3.1/all"
    try:
        r = requests.get(url, timeout=8)
        r.raise_for_status()
        data = r.json()
        out = []
        for c in data:
            # ensure structure exists
            name = c.get("name", {}).get("common")
            code = c.get("cca2") or c.get("cca3") or None
            if name and code:
                out.append({"name": name, "code": code})
        if not out:
            # unexpected schema
            print("get_all_countries: empty result, using fallback")
            return FALLBACK
        # sort by name
        out = sorted(out, key=lambda x: x["name"])
        return out
    except Exception as e:
        # print to stdout so Streamlit logs show it
        print("get_all_countries() ERROR:", repr(e))
        # return fallback so UI still works
        return FALLBACK
# ---------- end helper ----------
# ---------- City search helper (GLOBAL) ----------
def search_cities(country_code, city_name, limit=10):
    """
    Uses Open-Meteo geocoding to search cities globally by country.
    Returns: [{name, admin1, lat, lon}, ...]
    """
    if not city_name:
        return []

    try:
        r = requests.get(
            "https://geocoding-api.open-meteo.com/v1/search",
            params={
                "name": city_name,
                "count": limit,
                "language": "en",
                "format": "json",
                "country": country_code,
            },
            timeout=8,
        )
        r.raise_for_status()

        results = r.json().get("results", [])
        return [
            {
                "name": x.get("name"),
                "admin1": x.get("admin1"),
                "lat": x.get("latitude"),
                "lon": x.get("longitude"),
            }
            for x in results
            if x.get("latitude") and x.get("longitude")
        ]
    except Exception as e:
        print("search_cities ERROR:", repr(e))
        return []
# ---------- end helper ----------

# ---------- Weather helpers (Open-Meteo) ----------
@lru_cache(maxsize=128)
def fetch_weather(lat, lon, months):
    """
    Historical daily weather for the past N months
    """
    end = date.today()
    start = end - timedelta(days=months * 30)

    r = requests.get(
        "https://archive-api.open-meteo.com/v1/archive",
        params={
            "latitude": lat,
            "longitude": lon,
            "start_date": start.isoformat(),
            "end_date": end.isoformat(),
            "daily": "precipitation_sum,temperature_2m_mean",
            "timezone": "UTC",
        },
        timeout=10,
    )
    r.raise_for_status()
    d = r.json()["daily"]

    df = pd.DataFrame({
        "date": pd.to_datetime(d["time"]),
        "rainfall_mm": d["precipitation_sum"],
        "temperature_c": d["temperature_2m_mean"],
    })

    return df.dropna().reset_index(drop=True)


@lru_cache(maxsize=128)
def fetch_forecast_weather(lat, lon, days):
    """
    Short-term daily forecast (Open-Meteo limit ~14 days)
    """
    days = min(days, 14)  # API limit

    r = requests.get(
        "https://api.open-meteo.com/v1/forecast",
        params={
            "latitude": lat,
            "longitude": lon,
            "daily": "precipitation_sum,temperature_2m_mean",
            "forecast_days": days,
            "timezone": "UTC",
        },
        timeout=10,
    )
    r.raise_for_status()
    d = r.json()["daily"]

    df = pd.DataFrame({
        "date": pd.to_datetime(d["time"]),
        "rainfall_mm": d["precipitation_sum"],
        "temperature_c": d["temperature_2m_mean"],
    })

    return df.dropna().reset_index(drop=True)
# ---------- end weather helpers ----------

st.markdown("""
<style>
.kpi-card {
    background-color: #000000;
    padding: 18px 20px;
    border-radius: 14px;
    box-shadow: 0 4px 14px rgba(0,0,0,0.35);
    min-height: 110px;
}
.kpi-label {
    color: #9ca3af;
    font-size: 0.85rem;
    margin-bottom: 6px;
}
.kpi-value {
    font-size: 1.8rem;
    font-weight: 700;
}
.kpi-caption {
    color: #d1d5db;
    font-size: 0.85rem;
    margin-top: 6px;
}
.blue { color: #3b82f6; }
.green { color: #22c55e; }
.orange { color: #f97316; }
.red { color: #ef4444; }
.cyan { color: #06b6d4; }
</style>
""", unsafe_allow_html=True)


# =============================================================

# =========================================================
# üîê PRICING PLANS & FEATURE ACCESS
# =========================================================

PLANS = {
    "starter": {
        "analytics": False,
        "business_intelligence": False,
        "max_leads_per_month": 50,
        "max_users": 1,
    },
    "pro": {
        "analytics": True,
        "business_intelligence": True,
        "max_leads_per_month": 300,
        "max_users": 5,
    },
    "business": {
        "analytics": True,
        "business_intelligence": True,
        "max_leads_per_month": 2000,
        "max_users": 10,
    },
    "enterprise": {"max_leads_per_month": None},
}





# ----------------------
# CONFIG
# ----------------------
APP_TITLE = "ReCapture Pro"
DB_FILE = "titan_backend.db"   # stored in app working directory
MODEL_FILE = "titan_model.joblib"
PIPELINE_STAGES = [
    "New", "Contacted", "Inspection Scheduled", "Inspection Completed",
    "Estimate Sent", "Qualified", "Won", "Lost"
]
DEFAULT_SLA_HOURS = 72
COMFORTAA_IMPORT = "https://fonts.googleapis.com/css2?family=Comfortaa:wght@300;400;700&display=swap"


# KPI colors (numbers)
KPI_COLORS = ["#2563eb", "#0ea5a4", "#a855f7", "#f97316", "#ef4444", "#6d28d9", "#22c55e"]


#-----------------------
# STRIPE DUMMY TEST
#-----------------------
STRIPE_ENABLED = False  # turn ON later
STRIPE_PLANS = {
    "starter": None,
    "pro": "price_test_pro",
    "business": "price_test_business",
}


# ----------------------
# DB SETUP
# ----------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_PATH = os.path.join(BASE_DIR, DB_FILE)
ENGINE_URL = f"sqlite:///{DB_PATH}"

engine = create_engine(
    ENGINE_URL,
    connect_args={"check_same_thread": False}
)
SessionLocal = sessionmaker(bind=engine, expire_on_commit=False)
Base = declarative_base()


# ----------------------
# MODELS
# ----------------------
class User(Base):
    __tablename__ = "users"

    id = Column(Integer, primary_key=True)

    # PRIMARY IDENTITY
    email = Column(String, unique=True, nullable=False, index=True)
    email_verified = Column(Boolean, default=False)

    # OPTIONAL / INTERNAL
    username = Column(String, unique=True, nullable=True)
    full_name = Column(String, default="")
    role = Column(String, default="Admin")

    created_at = Column(DateTime, default=datetime.utcnow)

    # BILLING / ACCESS
    plan = Column(String, default="starter")
    trial_ends_at = Column(DateTime, nullable=True)
    subscription_status = Column(String, default="trial")

    # =========================
    # ACCOUNT STATUS (NEW)
    # =========================
    is_active = Column(Boolean, default=True)
    last_login_at = Column(DateTime, nullable=True)


    # ----------------------
    # AUTH CONFIG
    # ----------------------
    JWT_SECRET = os.environ.get("JWT_SECRET", "CHANGE_ME_NOW")
    JWT_ALGO = "HS256"
    JWT_EXP_MINUTES = 15

    password_hash = Column(String, nullable=True)



class UserInvite(Base):
    __tablename__ = "user_invites"

    id = Column(Integer, primary_key=True)
    email = Column(String, nullable=False, index=True)
    token = Column(String, unique=True, nullable=False)
    role = Column(String, default="Staff")
    invited_by = Column(String, nullable=True)
    expires_at = Column(DateTime, nullable=False)
    accepted = Column(Boolean, default=False)
    created_at = Column(DateTime, default=datetime.utcnow)


class LoginToken(Base):
    __tablename__ = "login_tokens"

    id = Column(Integer, primary_key=True)
    token = Column(String, unique=True, nullable=False, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False)
    expires_at = Column(DateTime, nullable=False)
    used = Column(Boolean, default=False)

    user = relationship("User")

class Lead(Base):
    __tablename__ = "leads"
    id = Column(Integer, primary_key=True)
    lead_id = Column(String, unique=True, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    source = Column(String, default="Other")
    source_details = Column(String, nullable=True)
    contact_name = Column(String, nullable=True)
    contact_phone = Column(String, nullable=True)
    contact_email = Column(String, nullable=True)
    property_address = Column(String, nullable=True)
    damage_type = Column(String, nullable=True)
    assigned_to = Column(String, nullable=True)  # username of owner
    notes = Column(Text, nullable=True)
    estimated_value = Column(Float, default=0.0)
    stage = Column(String, default="New")
    sla_hours = Column(Integer, default=DEFAULT_SLA_HOURS)
    sla_entered_at = Column(DateTime, default=datetime.utcnow)
    contacted = Column(Boolean, default=False)
    inspection_scheduled = Column(Boolean, default=False)
    inspection_scheduled_at = Column(DateTime, nullable=True)
    inspection_completed = Column(Boolean, default=False)
    estimate_submitted = Column(Boolean, default=False)
    estimate_submitted_at = Column(DateTime, nullable=True)
    awarded_date = Column(DateTime, nullable=True)
    awarded_invoice = Column(String, nullable=True)
    lost_date = Column(DateTime, nullable=True)
    qualified = Column(Boolean, default=False)
    ad_cost = Column(Float, default=0.0)  # cost to acquire
    converted = Column(Boolean, default=False)
    score = Column(Float, nullable=True)  # ML probability


class LeadHistory(Base):
    __tablename__ = "lead_history"
    id = Column(Integer, primary_key=True)
    lead_id = Column(String, nullable=False)
    changed_by = Column(String, nullable=True)
    field = Column(String, nullable=True)
    old_value = Column(String, nullable=True)
    new_value = Column(String, nullable=True)
    timestamp = Column(DateTime, default=datetime.utcnow)
    # ---------- BEGIN BLOCK A: NEW MODELS (Technician, InspectionAssignment, LocationPing) ----------
from sqlalchemy import DateTime as SA_DateTime


class Technician(Base):
    __tablename__ = "technicians"

    id = Column(Integer, primary_key=True)
    username = Column(String, unique=True, nullable=False)
    full_name = Column(String, default="")
    phone = Column(String, nullable=True)
    specialization = Column(String, nullable=True)

    # ‚úÖ ADD THIS LINE
    status = Column(String, default="available")  
    # available, assigned, enroute, onsite, completed

    active = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)



class InspectionAssignment(Base):
    __tablename__ = "inspection_assignments"
    id = Column(Integer, primary_key=True)
    lead_id = Column(String, nullable=False)          # lead_id from Lead.lead_id
    technician_username = Column(String, nullable=False)
    assigned_at = Column(DateTime, default=datetime.utcnow)
    status = Column(String, default="assigned")      # assigned, enroute, onsite, completed, cancelled
    notes = Column(Text, nullable=True)


class LocationPing(Base):
    __tablename__ = "location_pings"
    id = Column(Integer, primary_key=True)
    tech_username = Column(String, nullable=False)
    lead_id = Column(String, nullable=True)          # optional - link to lead if assigned
    latitude = Column(Float, nullable=False)
    longitude = Column(Float, nullable=False)
    timestamp = Column(DateTime, default=datetime.utcnow)
    accuracy = Column(Float, nullable=True)          # optional accuracy (meters)

class Task(Base):
    __tablename__ = "tasks"
    id = Column(Integer, primary_key=True)
    lead_id = Column(String, nullable=True)
    technician_username = Column(String, nullable=True)
    title = Column(String, nullable=False)
    description = Column(Text, nullable=True)
    status = Column(String, default="open")  # open, in_progress, done
    due_at = Column(DateTime, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)

# ---------- BEGIN BLOCK A2: COMPETITOR INTELLIGENCE MODELS ----------

class Competitor(Base):
    __tablename__ = "competitors"

    id = Column(Integer, primary_key=True)
    name = Column(String, nullable=False)
    place_id = Column(String, unique=True, nullable=True)
    latitude = Column(Float, nullable=True)
    longitude = Column(Float, nullable=True)
    rating = Column(Float, default=0.0)
    total_reviews = Column(Integer, default=0)
    primary_category = Column(String, nullable=True)
    service_area = Column(String, nullable=True)
    active = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)


class CompetitorSnapshot(Base):
    __tablename__ = "competitor_snapshots"

    id = Column(Integer, primary_key=True)
    competitor_id = Column(Integer, ForeignKey("competitors.id"))
    rating = Column(Float)
    total_reviews = Column(Integer)
    captured_at = Column(DateTime, default=datetime.utcnow)

    competitor = relationship("Competitor")

class CompetitorAlert(Base):
    __tablename__ = "competitor_alerts"

    id = Column(Integer, primary_key=True)
    competitor_id = Column(Integer)
    alert_type = Column(String)
    message = Column(String)
    severity = Column(String)  # low / medium / high
    created_at = Column(DateTime, default=datetime.utcnow)

# ---------- END BLOCK A2 ----------
Base.metadata.create_all(bind=engine)

# ---------- END BLOCK A ----------




# Create tables if missing
from sqlalchemy import inspect

inspector = inspect(engine)


def safe_create_tables():
    inspector = inspect(engine)
    existing_tables = inspector.get_table_names()


    for table in Base.metadata.sorted_tables:
        if table.name not in existing_tables:
            table.create(bind=engine)


safe_create_tables()




# Safe migration attempt (best-effort add missing columns)
def safe_migrate():
    try:
        inspector = inspect(engine)

        if "users" in inspector.get_table_names():
            existing = [c["name"] for c in inspector.get_columns("users")]
            desired = {
                "plan": "TEXT",
                "trial_ends_at": "DATETIME",
                "subscription_status": "TEXT",
            }

            with engine.begin() as conn:
                for col, typ in desired.items():
                    if col not in existing:
                        conn.execute(
                            text(f"ALTER TABLE users ADD COLUMN {col} {typ}")
                        )

    except Exception as e:
        print("‚ö†Ô∏è User migration skipped:", e)

safe_migrate()

def create_login_token(user, minutes=15):
    token = secrets.token_urlsafe(32)

    login_token = LoginToken(
        token=token,
        user_id=user.id,
        expires_at=datetime.utcnow() + timedelta(minutes=minutes),
    )

    with SessionLocal() as s:
        s.add(login_token)
        s.commit()

    return token

def verify_login_token(token: str):
    with SessionLocal() as s:
        login_token = (
            s.query(LoginToken)
            .filter(
                LoginToken.token == token,
                LoginToken.used == False,
                LoginToken.expires_at > datetime.utcnow(),
            )
            .first()
        )

        if not login_token:
            return None

        login_token.used = True
        login_token.user.last_login_at = datetime.utcnow()
        s.commit()

        return login_token.user

from datetime import datetime

def verify_login_token(token: str):
    with SessionLocal() as s:
        login_token = (
            s.query(LoginToken)
            .filter(
                LoginToken.token == token,
                LoginToken.used == False,
                LoginToken.expires_at > datetime.utcnow(),
            )
            .first()
        )

        if not login_token:
            return None

        login_token.used = True
        login_token.user.last_login_at = datetime.utcnow()
        s.commit()

        return login_token.user


def login_user(user):
    st.session_state["user_id"] = user.id
    st.session_state["user_role"] = user.role
    st.session_state["user_email"] = user.email


def logout_user():
    st.session_state.clear()




from sqlalchemy import inspect, text

from sqlalchemy import inspect, text

def safe_migrate_new_tables():
    try:
        inspector = inspect(engine)

        # Ensure tables exist
        Base.metadata.create_all(engine)

        with engine.begin() as conn:

            # ---- USERS TABLE ----
            if "users" in inspector.get_table_names():
                cols = [c["name"] for c in inspector.get_columns("users")]

                if "email" not in cols:
                    conn.execute(text("ALTER TABLE users ADD COLUMN email VARCHAR"))

                if "email_verified" not in cols:
                    conn.execute(text("ALTER TABLE users ADD COLUMN email_verified BOOLEAN DEFAULT 0"))

                if "is_active" not in cols:
                    conn.execute(text("ALTER TABLE users ADD COLUMN is_active BOOLEAN DEFAULT 1"))

                if "last_login_at" not in cols:
                    conn.execute(text("ALTER TABLE users ADD COLUMN last_login_at DATETIME"))
                
                #-------- Extension ------------------------------------------
                if "invite_token_hash" not in cols:
                    conn.execute(text("ALTER TABLE users ADD COLUMN invite_token_hash VARCHAR"))
                
                if "invite_expires_at" not in cols:
                    conn.execute(text("ALTER TABLE users ADD COLUMN invite_expires_at DATETIME"))
                
                if "activated_at" not in cols:
                    conn.execute(text("ALTER TABLE users ADD COLUMN activated_at DATETIME"))


            # ---- TECHNICIANS TABLE ----
            if "technicians" in inspector.get_table_names():
                cols = [c["name"] for c in inspector.get_columns("technicians")]
                if "status" not in cols:
                    conn.execute(
                        text("ALTER TABLE technicians ADD COLUMN status VARCHAR DEFAULT 'available'")
                    )
                        # ---- USERS TABLE ----
            if "users" in inspector.get_table_names():
                cols = [c["name"] for c in inspector.get_columns("users")]
            
                if "activation_token" not in cols:
                    conn.execute(
                        text("ALTER TABLE users ADD COLUMN activation_token VARCHAR")
                    )
            
                if "activation_expires_at" not in cols:
                    conn.execute(
                        text("ALTER TABLE users ADD COLUMN activation_expires_at DATETIME")
                    )
                        # ---- PASSWORD RESET ----
            if "password_reset_token" not in cols:
                conn.execute(
                    text("ALTER TABLE users ADD COLUMN password_reset_token VARCHAR")
                )
            
            if "password_reset_expires_at" not in cols:
                conn.execute(
                    text("ALTER TABLE users ADD COLUMN password_reset_expires_at DATETIME")
                )



    except Exception as e:
        print("Safe migration skipped:", e)



# Call migration
safe_migrate_new_tables()


def haversine_km(lat1, lon1, lat2, lon2):
    """
    Calculate distance between two lat/lon points in KM
    """
    R = 6371
    phi1, phi2 = math.radians(lat1), math.radians(lat2)
    dphi = math.radians(lat2 - lat1)
    dlambda = math.radians(lon2 - lon1)

    a = math.sin(dphi / 2) ** 2 + \
        math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2) ** 2

    return 2 * R * math.atan2(math.sqrt(a), math.sqrt(1 - a))



# ---------- BEGIN BLOCK D: COMPETITOR HELPERS ----------

@st.cache_data(ttl=86400)
def calculate_competitor_score(rating, reviews, distance_km):
    if distance_km <= 0:
        distance_km = 1
    return round((rating * reviews) / distance_km, 2)


def save_competitor_snapshot(competitor_id, rating, total_reviews):
    s = get_session()
    try:
        snap = CompetitorSnapshot(
            competitor_id=competitor_id,
            rating=rating,
            total_reviews=total_reviews
        )
        s.add(snap)
        s.commit()
    except Exception:
        s.rollback()
    finally:
        s.close()
def seo_visibility_gap(you_reviews, you_rating, competitors_df):
    avg_comp_reviews = competitors_df["Reviews"].mean()
    avg_comp_rating = competitors_df["Rating"].mean()

    review_gap = avg_comp_reviews - you_reviews
    rating_gap = avg_comp_rating - you_rating

    return {
        "review_gap": round(review_gap, 1),
        "rating_gap": round(rating_gap, 2),
        "pressure": "HIGH" if review_gap > 20 or rating_gap > 0.3 else "MODERATE"
    }

# ---------- END BLOCK D ----------
# ---------- BEGIN BLOCK F: GOOGLE PLACES INGESTION ----------

import requests
from datetime import datetime

OVERPASS_URL = "https://overpass-api.de/api/interpreter"


import requests

def ingest_competitors_openstreetmap(lat, lon, keyword, radius=5000):
    """
    Fetch competitors from OpenStreetMap around a point using Overpass API.
    """
    overpass_url = "https://overpass-api.de/api/interpreter"
    # Replace spaces in keyword for OSM query
    keyword = keyword.lower().replace(" ", "_")

    query = f"""
    [out:json][timeout:25];
    node
      ["name"]
      ["amenity"]
      (around:{radius},{lat},{lon});
    out center;
    """

    try:
        response = requests.get(overpass_url, params={"data": query})
        response.raise_for_status()
        data = response.json()
        elements = data.get("elements", [])

        if not elements:
            st.warning("No competitors found in this area.")
            return

        s = get_session()
        try:
            for e in elements:
                name = e.get("tags", {}).get("name")
                category = e.get("tags", {}).get("amenity", "unknown")
                lat_ = e.get("lat") or e.get("center", {}).get("lat")
                lon_ = e.get("lon") or e.get("center", {}).get("lon")
                if name and lat_ and lon_:
                    # Avoid duplicates
                    exists = s.query(Competitor).filter_by(name=name).first()
                    if not exists:
                        comp = Competitor(
                            name=name,
                            primary_category=category,
                            latitude=lat_,
                            longitude=lon_,
                            total_reviews=0,
                            rating=0.0,
                        )
                        s.add(comp)
            s.commit()
        finally:
            s.close()

    except Exception as e:
        raise RuntimeError(f"OSM competitor scan failed: {e}")


# ---------- END BLOCK F ----------
def review_velocity(competitor_id, days):
    s = get_session()
    try:
        since = datetime.utcnow() - timedelta(days=days)
        count = (
            s.query(CompetitorSnapshot)
            .filter(
                CompetitorSnapshot.competitor_id == competitor_id,
                CompetitorSnapshot.captured_at >= since
            )
            .count()
        )
        return round(count / max(days, 1), 2)
    finally:
        s.close()
def generate_competitor_alerts():
    s = get_session()
    try:
        competitors = s.query(Competitor).all()

        for c in competitors:
            v7 = review_velocity(c.id, 7)
            v30 = review_velocity(c.id, 30)

            if v7 >= 10:
                s.add(CompetitorAlert(
                    competitor_id=c.id,
                    alert_type="REVIEW_SPIKE",
                    severity="high",
                    message=f"{c.name} gained {v7} reviews in 7 days."
                ))

            if v30 >= 25:
                s.add(CompetitorAlert(
                    competitor_id=c.id,
                    alert_type="AGGRESSIVE_GROWTH",
                    severity="high",
                    message=f"{c.name} gained {v30} reviews in 30 days."
                ))

        s.commit()
    finally:
        s.close()


# ----------------------
# HELPERS: DB ops
# ----------------------
def get_session():
    return SessionLocal()


def leads_to_df(start_date=None, end_date=None):
    """Load leads into a DataFrame. Filter by optional start_date/end_date (date objects)"""
    s = get_session()
    try:
        rows = s.query(Lead).order_by(Lead.created_at.desc()).all()
        data = []
        for r in rows:
            data.append({
                "id": r.id,
                "lead_id": r.lead_id,
                "created_at": r.created_at,
                "source": r.source or "Other",
                "source_details": getattr(r, "source_details", None),
                "contact_name": getattr(r, "contact_name", None),
                "contact_phone": getattr(r, "contact_phone", None),
                "contact_email": getattr(r, "contact_email", None),
                "property_address": getattr(r, "property_address", None),
                "damage_type": getattr(r, "damage_type", None),
                "assigned_to": getattr(r, "assigned_to", None),
                "notes": r.notes,
                "estimated_value": float(r.estimated_value or 0.0),
                "stage": r.stage or "New",
                "sla_hours": int(r.sla_hours or DEFAULT_SLA_HOURS),
                "sla_entered_at": r.sla_entered_at or r.created_at,
                "contacted": bool(r.contacted),
                "inspection_scheduled": bool(r.inspection_scheduled),
                "inspection_scheduled_at": r.inspection_scheduled_at,
                "inspection_completed": bool(r.inspection_completed),
                "estimate_submitted": bool(r.estimate_submitted),
                "awarded_date": r.awarded_date,
                "lost_date": r.lost_date,
                "qualified": bool(r.qualified),
                "ad_cost": float(r.ad_cost or 0.0),
                "converted": bool(r.converted),
                "score": float(r.score) if r.score is not None else None
            })
        df = pd.DataFrame(data)
        if df.empty:
            # return empty with expected columns
            cols = ["id","lead_id","created_at","source","source_details","contact_name","contact_phone","contact_email",
                    "property_address","damage_type","assigned_to","notes","estimated_value","stage","sla_hours","sla_entered_at",
                    "contacted","inspection_scheduled","inspection_scheduled_at","inspection_completed","estimate_submitted",
                    "awarded_date","lost_date","qualified","ad_cost","converted","score"]
            return pd.DataFrame(columns=cols)
        # apply date filters
        if start_date:
            start_dt = datetime.combine(start_date, datetime.min.time())
            df = df[df["created_at"] >= start_dt]
        if end_date:
            end_dt = datetime.combine(end_date, datetime.max.time())
            df = df[df["created_at"] <= end_dt]
        return df.reset_index(drop=True)
    finally:
        s.close()

def set_logged_in_user(user: User):
    st.session_state["user_id"] = user.id

    
# ----------------------
# AUTH HELPERS
# ----------------------

def get_current_user():
    """
    Resolves the currently authenticated user.
    Enforces:
    - account is active
    - email is verified
    """

    user_id = st.session_state.get("user_id")
    if not user_id:
        return None

    with SessionLocal() as s:
        user = s.query(User).filter(User.id == user_id).first()

        if not user:
            st.session_state.clear()
            st.error("User session invalid")
            st.stop()

        if not user.is_active:
            st.error("Your account is inactive. Please contact an administrator.")
            st.stop()

        if not user.email_verified:
            st.error("Please verify your email address before accessing the app.")
            st.stop()

        return user





def decode_wp_token(token: str):
    try:
        payload = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALGO])
        return payload
    except jwt.ExpiredSignatureError:
        return None
    except jwt.InvalidTokenError:
        return None
        
def generate_invite_token():
    return secrets.token_urlsafe(32)



def verify_invite_token(token: str) -> User:
    try:
        payload = jwt.decode(token, INVITE_SECRET, algorithms=["HS256"])
        email = payload.get("email")

        token_hash = hashlib.sha256(token.encode()).hexdigest()

        with SessionLocal() as s:
            user = (
                s.query(User)
                .filter(
                    User.email == email,
                    User.invite_token_hash == token_hash,
                    User.invite_expires_at > datetime.utcnow(),
                    User.activated_at.is_(None),
                )
                .first()
            )

            if not user:
                raise ValueError("Invalid or expired invite")

            # Activate user
            user.is_active = True
            user.activated_at = datetime.utcnow()
            user.invite_token_hash = None
            user.invite_expires_at = None

            s.commit()
            return user

    except jwt.ExpiredSignatureError:
        raise ValueError("Invite expired")


# ---------- BEGIN BLOCK C: DB HELPERS FOR TECHNICIANS / ASSIGNMENTS / PINGS ----------
def create_task(title, technician_username=None, lead_id=None, due_at=None, description=None):
    s = get_session()
    try:
        task = Task(
            title=title,
            technician_username=technician_username,
            lead_id=lead_id,
            description=description,
            status="open",
            due_at=due_at
        )
        s.add(task)
        s.commit()
    except Exception:
        s.rollback()
        raise
    finally:
        s.close()
def update_task_status(task_id: int, new_status: str):
    s = get_session()
    try:
        task = s.query(Task).filter(Task.id == task_id).first()
        if not task:
            return False
        task.status = new_status
        s.add(task)
        s.commit()
        return True
    except Exception:
        s.rollback()
        raise
    finally:
        s.close()

def get_tasks_for_user(username):
    s = get_session()
    try:
        rows = s.query(Task).filter(Task.technician_username == username).all()
        return pd.DataFrame([
            {
                "id": r.id,
                "title": r.title,
                "status": r.status,
                "lead_id": r.lead_id,
                "due_at": r.due_at
            } for r in rows
        ])
    finally:
        s.close()
def page_tasks():
    require_role_access("tasks")
    st.markdown("## ‚úÖ Technician Tasks")

    techs = get_technicians_df(active_only=True)

    if techs.empty:
        st.warning("No technicians available.")
        return

    tech_username = st.selectbox(
        "Select Technician",
        techs["username"].tolist()
    )

    tasks_df = get_tasks_for_user(tech_username)

    if tasks_df.empty:
        st.info(
    "‚ö†Ô∏è No task assigned to a Technician yet! To assign a job task to a technician, go to: SETTINGS at the Navigation Menu, then click on the TECHNICIAN MANAGEMENT."
)

        return

    for _, row in tasks_df.iterrows():
        with st.expander(f"üßæ {row['title']} ‚Äî {row['status'].upper()}"):
            st.write(f"**Lead ID:** {row['lead_id'] or 'N/A'}")
            st.write(f"**Due:** {row['due_at'] or 'No due date'}")

            if row["status"] == "open":
                if st.button("‚ñ∂Ô∏è Start Task", key=f"start_{row['id']}"):
                    update_task_status(row["id"], "in_progress")
                    st.success("Task started")
                    st.rerun()

            elif row["status"] == "in_progress":
                if st.button("‚úÖ Mark Complete", key=f"done_{row['id']}"):
                    update_task_status(row["id"], "done")
                    st.success("Task completed")
                    st.rerun()

            elif row["status"] == "done":
                st.success("‚úî Completed")


def get_tasks_df():
    s = get_session()
    try:
        rows = s.query(Task).order_by(Task.created_at.desc()).all()
        return pd.DataFrame([
            {
                "id": r.id,
                "title": r.title,
                "technician_username": r.technician_username,
                "lead_id": r.lead_id,
                "status": r.status,
                "due_at": r.due_at,
                "created_at": r.created_at
            } for r in rows
        ])
    finally:
        s.close()


def add_technician(username: str, full_name: str = "", phone: str = "", specialization: str = "Tech", active: bool = True):
    s = get_session()
    try:
        existing = s.query(Technician).filter(Technician.username == username).first()
        if existing:
            existing.full_name = full_name
            existing.phone = phone
            existing.specialization = specialization
            existing.active = active
            s.add(existing); s.commit()
            return existing.username
        t = Technician(username=username, full_name=full_name, phone=phone, specialization=specialization, active=active)
        s.add(t); s.commit()
        return t.username
    except Exception:
        s.rollback()
        raise
    finally:
        s.close()
def update_technician_status(username: str, status: str):
    s = get_session()
    try:
        tech = s.query(Technician).filter_by(username=username).first()
        if not tech:
            return False
        tech.status = status
        s.commit()
        return True
    finally:
        s.close()

if "_save_location" in st.query_params:
    data = st.get_json()
    save_location_ping(
        data["username"],
        data["lat"],
        data["lon"],
        data.get("accuracy")
    )
    st.stop()

def save_location_ping(username, lat, lon, accuracy=None):
    s = get_session()
    try:
        ping = LocationPing(
            tech_username=username,
            latitude=float(lat),
            longitude=float(lon),
            accuracy=accuracy,
            timestamp=datetime.utcnow()
        )
        s.add(ping)
        s.commit()
    finally:
        s.close()


def get_technicians_df(active_only=True):
    s = get_session()
    try:
        q = s.query(Technician)
        if active_only:
            q = q.filter(Technician.active == True)

        rows = q.all()

        return pd.DataFrame([
            {
                "username": t.username,
                "full_name": t.full_name,
                "phone": t.phone,
                "specialization": t.specialization,
                "active": t.active
            }
            for t in rows
        ])
    finally:
        s.close()

def save_location_ping(
    tech_username: str,
    latitude: float,
    longitude: float,
    lead_id: str | None = None,
    accuracy: float | None = None,
):
    s = get_session()
    try:
        ping = LocationPing(
            tech_username=tech_username,
            latitude=latitude,
            longitude=longitude,
            lead_id=lead_id,
            accuracy=accuracy
        )
        s.add(ping)
        s.commit()
    except Exception:
        s.rollback()
        raise
    finally:
        s.close()



def get_leads_df():
    s = get_session()
    try:
        rows = s.query(Lead).all()

        if not rows:
            return pd.DataFrame()

        return pd.DataFrame([
            {
                "lead_id": r.lead_id,
                "stage": r.stage,
                "estimated_value": r.estimated_value or 0,
                "assigned_to": r.assigned_to,
                "score": r.score if r.score is not None else 0.5,
                "sla_hours": r.sla_hours,
                "sla_entered_at": r.sla_entered_at,
                "created_at": r.created_at,
                "source": r.source,
                "damage_type": r.damage_type
            }
            for r in rows
        ])
    finally:
        s.close()


def get_jobs_for_period(start_dt, end_dt):
    df = get_leads_df()  # existing function

    if df.empty:
        return df

    df["created_at"] = pd.to_datetime(df["created_at"], errors="coerce")
    df = df[(df["created_at"] >= start_dt) & (df["created_at"] <= end_dt)]

    return df

def compute_job_volume_metrics(df):
    return {
        "total_jobs": len(df),
        "job_types": safe_col(df, "job_type").value_counts().to_dict(),
        "lead_sources": safe_col(df, "lead_source").value_counts().to_dict(),
    }



def compute_revenue_metrics(df):
    revenue_col = safe_col(df, "estimated_value", default_dtype=float).fillna(0)

    total_revenue = revenue_col.sum()

    revenue_per_job = (
        total_revenue / len(df)
        if len(df) > 0 else 0
    )

    return {
        "total_revenue": float(total_revenue),
        "revenue_per_job": float(revenue_per_job),
    }

def compute_efficiency_metrics(df):
    revenue_col = safe_col(df, "estimated_value", default_dtype=float).fillna(0)

    jobs = len(df)
    total_revenue = revenue_col.sum()

    return {
        "jobs": jobs,
        "revenue_per_job": (
            total_revenue / jobs if jobs > 0 else 0
        ),
    }


def generate_synthetic_signals(df):
    signals = []

    revenue_col = safe_col(df, "estimated_value", default_dtype=float).fillna(0)
    job_type_col = safe_col(df, "job_type")

    if len(df) == 0:
        signals.append("üßä No job activity detected in this period.")
        return signals

    avg_revenue = revenue_col.mean()

    if avg_revenue < 500:
        signals.append("‚ö†Ô∏è High job volume but low average revenue per job.")

    dominant_job = job_type_col.value_counts().idxmax() if not job_type_col.dropna().empty else None

    if dominant_job:
        signals.append(f"üéØ Revenue is highly concentrated in '{dominant_job}' jobs.")

    return signals


def seasonal_baseline(all_jobs, start_date, end_date):
    same_months = list(range(start_date.month, end_date.month + 1))

    hist = all_jobs[
        all_jobs["created_at"].dt.month.isin(same_months)
    ]

    if hist.empty:
        return None

    return {
        "avg_jobs": hist.groupby(hist["created_at"].dt.year).size().mean(),
        "avg_revenue": hist.groupby(hist["created_at"].dt.year)["estimated_value"].sum().mean(),
        "avg_rev_per_job": hist["estimated_value"].mean()
    }


def shift_period(start, end):
    delta = end - start
    return start - delta, end - delta

def safe_col(df, col, default_dtype=None):
    if col not in df.columns:
        if default_dtype:
            return pd.Series(dtype=default_dtype)
        return pd.Series()
    s = df[col]
    if default_dtype:
        s = pd.to_numeric(s, errors="coerce")
    return s

def init_trial():
    if "trial_start" not in st.session_state:
        st.session_state["trial_start"] = datetime.utcnow()
        st.session_state["plan"] = "trial"

def is_trial_active(days=14):
    start = st.session_state.get("trial_start")
    if not start:
        return False
    return (datetime.utcnow() - start).days < days

def count_leads_this_month():
    start = datetime.utcnow().replace(day=1, hour=0, minute=0, second=0)
    end = datetime.utcnow()
    df = leads_to_df(start, end)
    return len(df)



def get_current_plan():
    user = st.session_state.get("user")
    if not user:
        return "starter"

    if user.subscription_status == "trial":
        if user.trial_ends_at and datetime.utcnow() > user.trial_ends_at:
            return "expired"

    return user.plan






def has_access(feature_key: str) -> bool:
    plan = get_current_plan()
    if plan == "expired":
        return False
    return PLANS.get(plan, {}).get(feature_key, False)

# ----------------------
# ROLE-BASED ACCESS CONTROL
# ----------------------

ROLE_PERMISSIONS = {
    "Admin": {
        "overview",
        "lead_capture",
        "pipeline",
        "analytics",
        "business_intelligence",
        "competitor_intelligence",
        "technicians",
        "settings",
    },
    "Manager": {
        "overview",
        "lead_capture",
        "pipeline",
        "analytics",
        "business_intelligence",
        "technicians",
    },
    "Staff": {
        "overview",
        "lead_capture",
        "pipeline",
    },
    "Viewer": {
        "overview",
        "pipeline",
    },
}

def require_role_access(page_key: str):
    user = get_current_user()
    if not user:
        st.error("Authentication required.")
        st.stop()

    allowed = ROLE_PERMISSIONS.get(user.role, set())
    if page_key not in allowed:
        st.warning("üö´ You do not have access to this page.")
        st.stop()

    
def analyze_job_types(df):
    if df.empty:
        return {}

    grouped = df.groupby("damage_type").agg(
        jobs=("id", "count"),
        revenue=("estimated_value", "sum"),
        avg_revenue=("estimated_value", "mean"),
        revenue_std=("estimated_value", "std")
    ).reset_index()

    total_jobs = grouped["jobs"].sum()
    total_revenue = grouped["revenue"].sum()

    grouped["job_share"] = grouped["jobs"] / total_jobs
    grouped["revenue_share"] = grouped["revenue"] / total_revenue
    grouped["volatility"] = grouped["revenue_std"].fillna(0)

    insights = []

    for _, r in grouped.iterrows():
        if r["job_share"] > 0.45:
            insights.append(f"‚ö†Ô∏è Over-dependence on **{r['damage_type']}** jobs")

        if r["job_share"] > 0.3 and r["avg_revenue"] < grouped["avg_revenue"].mean():
            insights.append(
                f"üí∏ **{r['damage_type']}** jobs are high-volume but low-value"
            )

        if r["volatility"] > grouped["volatility"].mean() * 1.5:
            insights.append(
                f"üìâ **{r['damage_type']}** revenue is highly volatile"
            )

    return {
        "table": grouped.sort_values("revenue", ascending=False),
        "insights": insights
    }



def pct_change(current, previous):
    """
    Safe percentage change calculation.
    Returns 0 if previous is zero or missing.
    """
    try:
        if previous in (0, None):
            return 0
        return ((current - previous) / previous) * 100
    except Exception:
        return 0


def generate_executive_narrative(data):
    narrative = []
    risk_flags = []

    volume = data.get("volume", {})
    revenue = data.get("revenue", {})
    efficiency = data.get("efficiency", {})

    # build narrative lines as dicts
    narrative.append({
        "text": "...",
        "confidence": 85
    })

    health_score = 82

    return {
        "lines": narrative,
        "risk_flags": risk_flags,
        "health_score": health_score,
        "version": "G-hardened-v1"
    }


def compute_business_intelligence(range_key, custom_start=None, custom_end=None):
    # -----------------------------
    # Current Period
    # -----------------------------
    start, end = resolve_time_window(range_key, custom_start, custom_end)
    df = get_jobs_for_period(start, end)

    # -----------------------------
    # Previous Period (Same Length)
    # -----------------------------
    prev_start, prev_end = shift_period(start, end)
    prev_df = get_jobs_for_period(prev_start, prev_end)

    # -----------------------------
    # Metrics (Current)
    # -----------------------------
    volume = compute_job_volume_metrics(df)
    revenue = compute_revenue_metrics(df)
    efficiency = compute_efficiency_metrics(df)

    # -----------------------------
    # Metrics (Previous)
    # -----------------------------
    prev_volume = compute_job_volume_metrics(prev_df)
    prev_revenue = compute_revenue_metrics(prev_df)
    prev_efficiency = compute_efficiency_metrics(prev_df)

    # -----------------------------
    # Trends / Deltas (%)
    # -----------------------------
    volume["trend"] = pct_change(
        volume.get("total_jobs", 0),
        prev_volume.get("total_jobs", 0)
    )

    revenue["trend"] = pct_change(
        revenue.get("total_revenue", 0),
        prev_revenue.get("total_revenue", 0)
    )

    efficiency["trend"] = pct_change(
        efficiency.get("revenue_per_job", 0),
        prev_efficiency.get("revenue_per_job", 0)
    )

    # -----------------------------
    # Synthetic Signals
    # -----------------------------
    signals = generate_synthetic_signals(df)

    # -----------------------------
    # Final Output Object
    # -----------------------------
    output = {
        "window": (start, end),
        "previous_window": (prev_start, prev_end),
        "volume": volume,
        "revenue": revenue,
        "efficiency": efficiency,
        "signals": signals,
        "raw_df": df,
        "prev_df": prev_df,
    }
    
    # -----------------------------
    # STEP GÔ∏è‚É£ ‚Äî Executive Narrative Engine
    # -----------------------------
    output["executive_narrative"] = generate_executive_narrative(output)
    
    # -----------------------------
    # STEP HÔ∏è‚É£ ‚Äî Revenue Leakage Detection
    # -----------------------------
    output["revenue_leakage"] = detect_revenue_leakage(output)
    
    # -----------------------------
    # STEP IÔ∏è‚É£ ‚Äî Strategic Signals Engine
    # -----------------------------
    output["strategic_signals"] = generate_strategic_signals(output)
    
    # -----------------------------
    # STEP KÔ∏è‚É£ ‚Äî Business Risk Score
    # -----------------------------
    output["business_risk_score"] = compute_business_risk_score(output)

    
    # -----------------------------
    # STEP JÔ∏è‚É£ ‚Äî What Changed & Why (Comparison)
    # -----------------------------
    if prev_df is not None and not prev_df.empty:
        previous_snapshot = {
            "volume": prev_volume,
            "revenue": prev_revenue,
            "efficiency": prev_efficiency,
        }
        output["what_changed"] = explain_what_changed(
            output,
            previous_snapshot
        )
    
    return output
    






def add_time_windows(df, date_col="date"):
    df[date_col] = pd.to_datetime(df[date_col])
    now = df[date_col].max()

    return {
        "3m": df[df[date_col] >= now - pd.DateOffset(months=3)],
        "6m": df[df[date_col] >= now - pd.DateOffset(months=6)],
        "12m": df[df[date_col] >= now - pd.DateOffset(months=12)],
    }
def generate_seasonal_insights(leads_df, hist_df):
    insights = []

    # Safety check
    if leads_df.empty or "damage_type" not in leads_df.columns:
        return ["‚ÑπÔ∏è No lead data available to generate insights."]

    # Example logic
    top_damage = leads_df["damage_type"].value_counts().idxmax()
    insights.append(f"Most common damage type among recent leads: {top_damage}")

    # You can add more insights based on hist_df
    avg_rain = hist_df["rainfall_mm"].mean()
    if avg_rain > 50:
        insights.append("üåßÔ∏è High rainfall observed, water damage risk elevated.")
    else:
        insights.append("üå§Ô∏è Rainfall is moderate, typical seasonal risk.")

    return insights



def create_inspection_assignment(lead_id: str, technician_username: str, notes: str = None):
    s = get_session()
    try:
        ia = InspectionAssignment(lead_id=lead_id, technician_username=technician_username, notes=notes)
        s.add(ia)
        # optional: also set lead.assigned_to to technician_username (non-destructive)
        lead = s.query(Lead).filter(Lead.lead_id == lead_id).first()
        if lead:
            lead.assigned_to = technician_username
            s.add(lead)
            s.add(LeadHistory(lead_id=lead.lead_id, changed_by="system", field="assigned_to", old_value=str(getattr(lead, "assigned_to", "")), new_value=str(technician_username)))
        s.commit()
        return ia.id
    except Exception:
        s.rollback()
        raise
    finally:
        s.close()


def get_assignments_for_lead(lead_id: str):
    s = get_session()
    try:
        rows = s.query(InspectionAssignment).filter(InspectionAssignment.lead_id == lead_id).order_by(InspectionAssignment.assigned_at.desc()).all()
        return rows
    finally:
        s.close()


def persist_location_ping(tech_username: str, latitude: float, longitude: float, lead_id: str = None, accuracy: float = None, timestamp: datetime = None):
    s = get_session()
    try:
        ping = LocationPing(tech_username=tech_username, latitude=float(latitude), longitude=float(longitude), lead_id=lead_id, accuracy=accuracy, timestamp=timestamp or datetime.utcnow())
        s.add(ping)
        s.commit()
        return ping.id
    except Exception:
        s.rollback()
        raise
    finally:
        s.close()

def get_latest_location_pings():
    s = get_session()
    try:
        rows = s.execute(
            text("""
                SELECT lp.tech_username, lp.latitude, lp.longitude, lp.timestamp
                FROM location_pings lp
                INNER JOIN (
                    SELECT tech_username, MAX(timestamp) AS max_ts
                    FROM location_pings
                    GROUP BY tech_username
                ) latest
                ON lp.tech_username = latest.tech_username
                AND lp.timestamp = latest.max_ts
            """)
        ).fetchall()

        return pd.DataFrame(rows, columns=[
            "tech_username", "latitude", "longitude", "timestamp"
        ])
    finally:
        s.close()



def classify_tech_status(ts):
    if ts is None:
        return "offline"

    if isinstance(ts, str):
        try:
            ts = pd.to_datetime(ts)
        except Exception:
            return "offline"

    now = datetime.utcnow()
    delta = (now - ts).total_seconds() / 60

    if delta <= 2:
        return "üü¢ Live"
    elif delta <= 10:
        return "üü° Idle"
    else:
        return "üî¥ Offline"
#-----------TIME WINDOW--------------

from datetime import datetime, timedelta

def resolve_time_window(range_key: str, custom_start=None, custom_end=None):
    now = datetime.utcnow()

    if range_key == "daily":
        return now - timedelta(days=1), now
    if range_key == "weekly":
        return now - timedelta(days=7), now
    if range_key == "30d":
        return now - timedelta(days=30), now
    if range_key == "90d":
        return now - timedelta(days=90), now
    if range_key == "6m":
        return now - timedelta(days=180), now
    if range_key == "12m":
        return now - timedelta(days=365), now
    if range_key == "custom" and custom_start and custom_end:
        return custom_start, custom_end

    # fallback
    return now - timedelta(days=30), now



# ----------------------
# EMAIL CONFIG
# ----------------------
SMTP_HOST = os.getenv("SMTP_HOST")
SMTP_PORT = int(os.getenv("SMTP_PORT", "587"))
SMTP_USER = os.getenv("SMTP_USER")
SMTP_PASSWORD = os.getenv("SMTP_PASSWORD")
EMAIL_FROM = os.getenv("EMAIL_FROM", SMTP_USER)




def send_email(to_email: str, subject: str, html_body: str):
    if not SMTP_HOST or not SMTP_USER or not SMTP_PASSWORD:
        raise RuntimeError("SMTP is not configured")

    msg = EmailMessage()
    msg["From"] = EMAIL_FROM
    msg["To"] = to_email
    msg["Subject"] = subject
    msg.set_content("This email requires HTML support.")
    msg.add_alternative(html_body, subtype="html")

    with smtplib.SMTP(SMTP_HOST, SMTP_PORT) as server:
        server.starttls()
        server.login(SMTP_USER, SMTP_PASSWORD)
        server.send_message(msg)

def send_invite_email(email: str, invite_link: str):
    html = f"""
    <div style="font-family: Arial, sans-serif;">
        <h2>You're invited to ReCapture Pro</h2>
        <p>You‚Äôve been invited to join <strong>ReCapture Pro</strong>.</p>
        <p>Your trial is ready. Click below to activate your account:</p>

        <p>
            <a href="{invite_link}"
               style="background:#2563eb;color:#fff;padding:12px 18px;
               text-decoration:none;border-radius:6px;">
               Activate Account
            </a>
        </p>

        <p>This link expires in 24 hours.</p>
    </div>
    """
    send_email(
        to_email=email,
        subject="You're invited to ReCapture Pro",
        html_body=html
    )



def generate_invite_token(email: str) -> tuple[str, str]:
    payload = {
        "email": email,
        "type": "invite",
        "exp": datetime.utcnow() + timedelta(hours=INVITE_EXPIRY_HOURS),
        "iat": datetime.utcnow(),
    }

    token = jwt.encode(payload, INVITE_SECRET, algorithm="HS256")

    token_hash = hashlib.sha256(token.encode()).hexdigest()

    return token, token_hash


def create_invite_user(email: str, role: str):
    token, token_hash = generate_invite_token(email)

    with SessionLocal() as s:
        user = User(
            email=email,
            username=email,
            role=role,
            is_active=False,
            invite_token_hash=token_hash,
            invite_expires_at=datetime.utcnow() + timedelta(hours=INVITE_EXPIRY_HOURS),
            plan="starter",
            subscription_status="trial",
            trial_ends_at=datetime.utcnow() + timedelta(days=14),
        )
        s.add(user)
        s.commit()

    return token


# ---------- END BLOCK C ----------




EMAIL_REGEX = re.compile(r"^[^@\s]+@[^@\s]+\.[^@\s]+$")

def is_valid_email(email: str) -> bool:
    if not email:
        return False
    return bool(EMAIL_REGEX.match(email.strip().lower()))


import secrets
from datetime import timedelta

def generate_activation_token():
    return secrets.token_urlsafe(32)

import secrets

def generate_password_reset_token():
    return secrets.token_urlsafe(32)


def upsert_lead_record(payload: dict, actor="admin"):
    """
    payload must include lead_id (string)
    other fields optional
    """
    s = get_session()
    try:
        lead = s.query(Lead).filter(Lead.lead_id == payload.get("lead_id")).first()
        if lead is None:
            # create
            lead = Lead(
                lead_id=payload.get("lead_id"),
                created_at=payload.get("created_at", datetime.utcnow()),
                source=payload.get("source"),
                source_details=payload.get("source_details"),
                contact_name=payload.get("contact_name"),
                contact_phone=payload.get("contact_phone"),
                contact_email=payload.get("contact_email"),
                property_address=payload.get("property_address"),
                damage_type=payload.get("damage_type"),
                assigned_to=payload.get("assigned_to"),
                notes=payload.get("notes"),
                estimated_value=float(payload.get("estimated_value") or 0.0),
                stage=payload.get("stage") or "New",
                sla_hours=int(payload.get("sla_hours") or DEFAULT_SLA_HOURS),
                sla_entered_at=payload.get("sla_entered_at") or datetime.utcnow(),
                ad_cost=float(payload.get("ad_cost") or 0.0),
                converted=bool(payload.get("converted") or False),
                score=payload.get("score")
            )
            s.add(lead)
            s.commit()
            s.add(LeadHistory(lead_id=lead.lead_id, changed_by=actor, field="create", old_value=None, new_value=str(lead.stage)))
            s.commit()
            return lead.lead_id
        else:
            # update fields and log changes
            changed = []
            for key in ["source","source_details","contact_name","contact_phone","contact_email","property_address",
                        "damage_type","assigned_to","notes","estimated_value","stage","sla_hours","sla_entered_at","ad_cost","converted","score"]:
                if key in payload:
                    new = payload.get(key)
                    old = getattr(lead, key)
                    # normalize numeric conversions
                    if key in ("estimated_value","ad_cost"):
                        try:
                            new_val = float(new or 0.0)
                        except Exception:
                            new_val = old
                    elif key in ("sla_hours",):
                        try:
                            new_val = int(new or old)
                        except Exception:
                            new_val = old
                    elif key in ("converted",):
                        new_val = bool(new)
                    else:
                        new_val = new
                    if new_val is not None and old != new_val:
                        changed.append((key, old, new_val))
                        setattr(lead, key, new_val)
            # persist
            s.add(lead)
            for (f, old, new) in changed:
                s.add(LeadHistory(lead_id=lead.lead_id, changed_by=actor, field=f, old_value=str(old), new_value=str(new)))
            s.commit()
            return lead.lead_id
    except Exception:
        s.rollback()
        raise
    finally:
        s.close()


def delete_lead_record(lead_id: str, actor="admin"):
    s = get_session()
    try:
        lead = s.query(Lead).filter(Lead.lead_id == lead_id).first()
        if not lead:
            return False
        s.add(LeadHistory(lead_id=lead.lead_id, changed_by=actor, field="delete", old_value=str(lead.stage), new_value="deleted"))
        s.delete(lead)
        s.commit()
        return True
    except Exception:
        s.rollback()
        raise
    finally:
        s.close()


def activate_user_from_token(token: str) -> bool:
    with SessionLocal() as s:
        user = (
            s.query(User)
            .filter(User.activation_token == token)
            .first()
        )

        if not user:
            return False

        if user.activation_expires_at and user.activation_expires_at < datetime.utcnow():
            return False

        # ‚úÖ Activate user -ACTIVATION TOKEN 
        user.is_active = True
        user.email_verified = True
        user.subscription_status = "active"
        user.activation_token = None
        user.activation_expires_at = None

        s.add(user)
        s.commit()
        return True


def get_users_df():
    try:
        with SessionLocal() as s:
            users = s.query(User).all()

        if not users:
            return pd.DataFrame(columns=[
                "username", "full_name", "role",
                "plan", "subscription_status", "created_at"
            ])

        return pd.DataFrame([
            {
                "username": u.username,
                "full_name": u.full_name,
                "role": u.role,
                "plan": getattr(u, "plan", "starter"),
                "subscription_status": getattr(u, "subscription_status", "trial"),
                "created_at": u.created_at,
            }
            for u in users
        ])

    except Exception as e:
        st.error("Failed to load users")
        st.code(str(e))
        return pd.DataFrame()

def get_leads_for_task_dropdown():
    """
    Returns DataFrame of leads for task assignment dropdown
    """
    s = get_session()
    try:
        rows = (
            s.query(
                Lead.lead_id,
                Lead.contact_name,
                Lead.property_address
            )
            .order_by(Lead.created_at.desc())
            .all()
        )

        return pd.DataFrame([
            {
                "lead_id": r.lead_id,
                "label": f"{r.lead_id} ‚Äî {r.contact_name or 'No Name'} ({r.property_address or 'No Address'})"
            }
            for r in rows
        ])
    finally:
        s.close()


from datetime import datetime, timedelta

def add_user(email, username=None, full_name=None, role="Staff"):
    # --- HARD BACKEND VALIDATION ---
    if not email:
        raise ValueError("Email is required")

    email = email.strip().lower()

    if not is_valid_email(email):
        raise ValueError("Invalid email format")

    with SessionLocal() as s:
        # --- UNIQUENESS CHECK ---
        existing = s.query(User).filter(User.email == email).first()
        if existing:
            raise ValueError("A user with this email already exists")

        user = User(
            email=email,
            username=username or email,
            full_name=full_name,
            role=role,
        )

        s.add(user)

        try:
            s.commit()
        except IntegrityError:
            s.rollback()
            raise ValueError("Email already exists")

        return user


def create_user_invite(email, role, invited_by):
    token = generate_invite_token()
    expires = datetime.utcnow() + timedelta(hours=48)

    with SessionLocal() as s:
        invite = UserInvite(
            email=email.lower(),
            token=token,
            role=role,
            invited_by=invited_by,
            expires_at=expires
        )
        s.add(invite)
        s.commit()

    return token

def accept_user_invite(token, username, full_name):
    with SessionLocal() as s:
        invite = s.query(UserInvite).filter(
            UserInvite.token == token,
            UserInvite.accepted == False,
            UserInvite.expires_at > datetime.utcnow()
        ).first()

        if not invite:
            return None, "Invalid or expired invite"

        user = User(
            email=invite.email,
            username=username,
            full_name=full_name,
            role=invite.role,
            plan="starter",
            subscription_status="trial",
            trial_ends_at=datetime.utcnow() + timedelta(days=14),
        )

        invite.accepted = True
        s.add(user)
        s.commit()

        return user, None



# ----------------------
# ML - internal only
# ----------------------
def train_internal_model():
    df = leads_to_df()
    if df.empty or df["converted"].nunique() < 2:
        return None, "Not enough labeled data to train"
    df2 = df.copy()
    df2["age_days"] = (datetime.utcnow() - df2["created_at"]).dt.days
    X = pd.get_dummies(df2[["source","stage"]].astype(str), drop_first=False)
    X["ad_cost"] = df2["ad_cost"]
    X["estimated_value"] = df2["estimated_value"]
    X["age_days"] = df2["age_days"]
    y = df2["converted"].astype(int)
    X = X.fillna(0)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    model = RandomForestClassifier(n_estimators=120, random_state=42)
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    acc = accuracy_score(y_test, preds)
    joblib.dump({"model": model, "columns": X.columns.tolist()}, MODEL_FILE)
    return acc, "trained"


def load_internal_model():
    if not os.path.exists(MODEL_FILE):
        return None, None
    try:
        obj = joblib.load(MODEL_FILE)
        return obj.get("model"), obj.get("columns")
    except Exception:
        return None, None


def score_dataframe(df, model, cols):
    if model is None or df.empty:
        df["score"] = np.nan
        return df
    df2 = df.copy()
    df2["age_days"] = (datetime.utcnow() - df2["created_at"]).dt.days
    X = pd.get_dummies(df2[["source","stage"]].astype(str), drop_first=False)
    X["ad_cost"] = df2["ad_cost"]
    X["estimated_value"] = df2["estimated_value"]
    X["age_days"] = df2["age_days"]
    for c in cols:
        if c not in X.columns:
            X[c] = 0
    X = X[cols].fillna(0)
    try:
        df["score"] = model.predict_proba(X)[:,1]
    except Exception:
        df["score"] = model.predict(X)
    return df


# ----------------------
# Priority & SLA utilities
# ----------------------
def calculate_remaining_sla(sla_entered_at, sla_hours):
    try:
        if sla_entered_at is None:
            sla_entered_at = datetime.utcnow()
        if isinstance(sla_entered_at, str):
            sla_entered_at = datetime.fromisoformat(sla_entered_at)
        deadline = sla_entered_at + timedelta(hours=int(sla_hours or DEFAULT_SLA_HOURS))
        remain = deadline - datetime.utcnow()
        return max(remain.total_seconds(), 0.0), (remain.total_seconds() <= 0)
    except Exception:
        return float("inf"), False


def compute_priority_for_row(row, weights=None):
    # row: Series/dict
    if weights is None:
        weights = {"score_w":0.6, "value_w":0.3, "sla_w":0.1, "value_baseline":5000.0}
    try:
        s = float(row.get("score") or 0.0)
    except Exception:
        s = 0.0
    try:
        val = float(row.get("estimated_value") or 0.0)
        vnorm = min(1.0, val / max(1.0, weights["value_baseline"]))
    except Exception:
        vnorm = 0.0
    try:
        sla_entered = row.get("sla_entered_at") or row.get("created_at")
        if sla_entered is None:
            sla_score = 0.0
        else:
            if isinstance(sla_entered, str):
                sla_entered = datetime.fromisoformat(sla_entered)
            time_left_h = max((sla_entered + timedelta(hours=row.get("sla_hours") or DEFAULT_SLA_HOURS) - datetime.utcnow()).total_seconds()/3600.0, 0.0)
            sla_score = max(0.0, (72.0 - min(time_left_h,72.0)) / 72.0)
    except Exception:
        sla_score = 0.0
    total = s*weights["score_w"] + vnorm*weights["value_w"] + sla_score*weights["sla_w"]
    return max(0.0, min(1.0, total))


# ----------------------
# UI CSS and layout
# ----------------------
st.set_page_config(page_title=APP_TITLE, layout="wide")
st.markdown(f"<link href='{COMFORTAA_IMPORT}' rel='stylesheet'>", unsafe_allow_html=True)


APP_CSS = """
<style>
body, .stApp { background: #ffffff; color: #0b1220; font-family: 'Comfortaa', sans-serif; }
.header { font-weight:800; font-size:20px; margin-bottom:6px; }
.kpi-grid { display:flex; gap:12px; flex-wrap:wrap; }
.kpi-card {
    background:#000;
    color:white;
    border-radius:12px;
    padding:12px;
    min-width:220px;
    box-shadow:0 8px 22px rgba(16,24,40,0.06);


    /* ‚úÖ add spacing */
    margin-top: 12px;
    margin-bottom: 12px;
}


.kpi-title { font-size:12px; opacity:0.95; margin-bottom:6px; }
.kpi-number { font-size:22px; font-weight:900; margin-bottom:8px; }
.progress-bar { height:8px; border-radius:8px; background:#e6e6e6; overflow:hidden; }
.progress-fill { height:100%; border-radius:8px; transition:width .4s ease; }
.lead-card {
    background: #000000; /* ‚úÖ Black */
    color: #ffffff; /* text stays white */
    border:1px solid #eef2ff;
    border-radius:10px;
    padding:12px;
    margin-bottom:8px;
}


.priority-time { color:#dc2626; font-weight:700; }
.priority-money { color:#22c55e; font-weight:800; }
.alert-bubble { background:#111; color:white; padding:10px; border-radius:10px; }
.small-muted { color:#F5F5F5; font-size:12px; }
</style>
"""
st.markdown(APP_CSS, unsafe_allow_html=True)


query_params = st.query_params

if "activate" in query_params:
    token = query_params.get("activate")

    st.title("üîê Activating your account...")

    success = activate_user_from_token(token)

    if success:
        st.success("‚úÖ Account activated successfully")
        st.info("You may now log in.")
    else:
        st.error("‚ùå Invalid or expired activation link")

    st.stop()






    st.markdown("---")
    st.markdown("Date range for reports")
    quick = st.selectbox("Quick range", ["Today","Last 7 days","Last 30 days","90 days","All","Custom"], index=4)
    if quick == "Today":
        st.session_state.start_date = date.today()
        st.session_state.end_date = date.today()
    elif quick == "Last 7 days":
        st.session_state.start_date = date.today() - timedelta(days=6)
        st.session_state.end_date = date.today()
    elif quick == "Last 30 days":
        st.session_state.start_date = date.today() - timedelta(days=29)
        st.session_state.end_date = date.today()
    elif quick == "90 days":
        st.session_state.start_date = date.today() - timedelta(days=89)
        st.session_state.end_date = date.today()
    elif quick == "All":
        st.session_state.start_date = None
        st.session_state.end_date = None
    else:
        sd, ed = st.date_input("Start / End", [date.today() - timedelta(days=29), date.today()])
        st.session_state.start_date = sd
        st.session_state.end_date = ed


    st.markdown("---")
    st.markdown("Internal ML runs silently. Use ML page to train/score.")
    if st.button("Refresh data"):
        # clear caches and refresh
        try:
            st.rerun()
        except Exception:
            pass


# Utility: date filters
start_dt = st.session_state.get("start_date", None)
end_dt = st.session_state.get("end_date", None)


# Load leads
try:
    leads_df = leads_to_df(start_dt, end_dt)
except OperationalError as exc:
    st.error("Database error ‚Äî ensure file is writable and accessible.")
    st.stop()


# Load model (if exists)
model, model_cols = load_internal_model()
if model is not None and not leads_df.empty:
    try:
        leads_df = score_dataframe(leads_df.copy(), model, model_cols)
    except Exception:
        # if scoring fails, continue without scores
        pass


# ----------------------
# Alerts bell (top-right)
# ----------------------
def alerts_ui():
    overdue = []
    for _, r in leads_df.iterrows():
        rem_s, overdue_flag = calculate_remaining_sla(
            r.get("sla_entered_at") or r.get("created_at"),
            r.get("sla_hours")
        )
        if overdue_flag and r.get("stage") not in ("Won", "Lost"):
            overdue.append(r)


    if overdue:
        col1, col2 = st.columns([1, 10])


        # BELL BUTTON (needs unique key)
        with col1:
            if st.button(f"üîî {len(overdue)}", key="alerts_button"):
                st.session_state.show_alerts = not st.session_state.get(
                    "show_alerts", False
                )


        with col2:
            st.markdown("")


        # EXPANDED ALERTS POPUP
        if st.session_state.get("show_alerts", False):
            with st.expander("SLA Alerts (click to close)", expanded=True):


                # list overdue leads
                for r in overdue:
                    st.markdown(
                        f"**{r['lead_id']}** ‚Äî Stage: {r['stage']} ‚Äî "
                        f"<span style='color:#22c55e;'>${r['estimated_value']:,.0f}</span> ‚Äî "
                        f"<span style='color:#dc2626;'>OVERDUE</span>",
                        unsafe_allow_html=True
                    )


                # CLOSE BUTTON (needs unique key)
                if st.button("Close Alerts", key="alerts_close_button"):
                    st.session_state.show_alerts = False


    else:
        st.markdown("")



def page_overview():
    import plotly.express as px
    st.markdown("<div class='header'>TOTAL LEAD PIPELINE ‚Äî KEY PERFORMANCE INDICATOR</div>", unsafe_allow_html=True)
    st.markdown("<em>High-level pipeline performance at a glance. Use filters and cards to drill into details.</em>", unsafe_allow_html=True)


    # Call alerts_ui() like your design; support both signatures
    try:
        alerts_ui()
    except TypeError:
        try:
            alerts_ui(leads_df if 'leads_df' in globals() else leads_to_df())
        except Exception:
            pass


    # Prefer leads_df if present, otherwise fall back to leads_to_df()
    try:
        if 'leads_df' in globals() and isinstance(leads_df, pd.DataFrame):
            df = leads_df.copy()
        else:
            df = leads_to_df()
    except Exception:
        df = leads_to_df()


    if not isinstance(df, pd.DataFrame):
        df = pd.DataFrame()


    total_leads = len(df)
    qualified_leads = int(df[df["qualified"] == True].shape[0]) if not df.empty else 0
    sla_success_count = int(df[df["contacted"] == True].shape[0]) if not df.empty else 0
    awarded_count = int(df[df["stage"] == "Won"].shape[0]) if not df.empty else 0
    lost_count = int(df[df["stage"] == "Lost"].shape[0]) if not df.empty else 0
    closed = awarded_count + lost_count
    conversion_rate = (awarded_count / closed * 100) if closed else 0.0
    inspection_count = int(df[df["inspection_scheduled"] == True].shape[0]) if not df.empty else 0
    inspection_pct = (inspection_count / qualified_leads * 100) if qualified_leads else 0.0
    estimate_sent_count = int(df[df["estimate_submitted"] == True].shape[0]) if not df.empty else 0
    pipeline_job_value = float(df["estimated_value"].sum()) if not df.empty else 0.0
    active_leads = total_leads - (awarded_count + lost_count)
    sla_success_pct = (sla_success_count / total_leads * 100) if total_leads else 0.0
    qualification_pct = (qualified_leads / total_leads * 100) if total_leads else 0.0


    try:
        KPI_COLORS
    except Exception:
        KPI_COLORS = ["#0ea5e9","#34d399","#f59e0b","#ef4444","#8b5cf6","#06b6d4","#10b981"]


    KPI_ITEMS = [
        ("Active Leads", f"{active_leads}", KPI_COLORS[0], "Leads currently in pipeline"),
        ("SLA Success", f"{sla_success_pct:.1f}%", KPI_COLORS[1], "Leads contacted within SLA"),
        ("Qualification Rate", f"{qualification_pct:.1f}%", KPI_COLORS[2], "Leads marked qualified"),
        ("Conversion Rate", f"{conversion_rate:.1f}%", KPI_COLORS[3], "Won / Closed"),
        ("Inspections Booked", f"{inspection_pct:.1f}%", KPI_COLORS[4], "Qualified ‚Üí Scheduled"),
        ("Estimates Sent", f"{estimate_sent_count}", KPI_COLORS[5], "Estimates submitted"),
        ("Pipeline Job Value", f"${pipeline_job_value:,.0f}", KPI_COLORS[6], "Total pipeline job value")
    ]


    # render 2 rows: first 4 then next 3
    r1 = st.columns(4)
    r2 = st.columns(3)
    cols = r1 + r2
    for col, (title, value, color, note) in zip(cols, KPI_ITEMS):
        pct = min(100, max(10, (abs(hash(title)) % 80) + 20))
        col.markdown(f"""
            <div class='kpi-card'>
              <div class='kpi-title'>{title}</div>
              <div class='kpi-number' style='color:{color};'>{value}</div>
              <div class='progress-bar'><div class='progress-fill' style='width:{pct}%; background:{color};'></div></div>
              <div class='small-muted'>{note}</div>
            </div>
        """, unsafe_allow_html=True)


    st.markdown("---")
    st.markdown("### Lead Pipeline Stages")
    st.markdown("<em>Distribution of leads across pipeline stages.</em>", unsafe_allow_html=True)


    if df.empty:
        st.info("No leads yet. Create one in Lead Capture.")
    else:
        try:
            stages = PIPELINE_STAGES
        except Exception:
            stages = ["New","Contacted","Inspection Scheduled","Inspection","Estimate Sent","Won","Lost"]
        stage_counts = df["stage"].value_counts().reindex(stages, fill_value=0)
        pie_df = pd.DataFrame({"status": stage_counts.index, "count": stage_counts.values})
        try:
            fig = px.pie(pie_df, names="status", values="count", hole=0.45, color="status")
            fig.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig, use_container_width=True)
        except Exception:
            st.bar_chart(stage_counts)


        st.markdown("---")


    # TOP 5 PRIORITY LEADS
    st.markdown("---")
    st.markdown("### TOP 5 PRIORITY LEADS")
    st.markdown("<em>Highest urgency leads by priority score (0‚Äì1). Address these first.</em>", unsafe_allow_html=True)
    if df.empty:
        st.info("No priority leads to display.")
    else:
        if 'compute_priority_for_row' in globals():
            scorer = compute_priority_for_row
        else:
            def scorer(r):
                rem_s, overdue = calculate_remaining_sla(r.get("sla_entered_at") or r.get("created_at"), r.get("sla_hours"))
                score = 0.0
                if overdue:
                    score += 0.6
                try:
                    score += min(0.4, float(r.get("estimated_value") or 0.0) / max(1.0, pipeline_job_value))
                except Exception:
                    pass
                return min(1.0, score)


        df = df.copy()
        df["priority_score"] = df.apply(lambda r: scorer(r), axis=1)
        pr_df = df.sort_values("priority_score", ascending=False).head(5)
        for _, r in pr_df.iterrows():
            sla_sec, overdue = calculate_remaining_sla(r.get("sla_entered_at") or r.get("created_at"), r.get("sla_hours"))
            hleft = int(sla_sec / 3600) if sla_sec not in (None, float("inf")) else 9999
            sla_html = f"<span class='priority-time'>‚ùó OVERDUE</span>" if overdue else f"<span class='small-muted'>‚è≥ {hleft}h left</span>"
            val_html = f"<span class='priority-money'>${r['estimated_value']:,.0f}</span>"
            st.markdown(f"""
                <div class='lead-card'>
                  <div style='display:flex; justify-content:space-between; align-items:center;'>
                    <div>
                      <div style='font-weight:800;'>#{r['lead_id']} ‚Äî {r.get('contact_name') or 'No name'}</div>
                      <div class='small-muted'>{r.get('damage_type') or ''} ‚Ä¢ {r.get('source') or ''}</div>
                    </div>
                    <div style='text-align:right;'>
                      <div style='font-size:20px; font-weight:900; color:#111;'>{r['priority_score']:.2f}</div>
                      <div style='margin-top:8px;'>{val_html}<br>{sla_html}</div>
                    </div>
                  </div>
                </div>
            """, unsafe_allow_html=True)


    st.markdown("---")
    st.markdown("### üìã All Leads (expand a card to edit / change status)")
    st.markdown("<em>Expand a lead to edit details, change status, assign owner, and create estimates.</em>", unsafe_allow_html=True)

        # ---- SAFELY PREPARE "stages" ----
    if not df.empty and "stage" in df.columns:
        stages = sorted(df["stage"].dropna().unique().tolist())
    else:
        stages = []
    # ---------------------------------
    # Quick filters
    q1, q2, q3 = st.columns([3,2,3])
    with q1:
        search_q = st.text_input("Search (lead_id, contact name, address, notes)", key="overview_search")
    with q2:
        filter_src = st.selectbox("Source filter", options=["All"] + sorted(df["source"].dropna().unique().tolist()) if not df.empty else ["All"], key="overview_filter_src")
    with q3:
        filter_stage = st.selectbox("Stage filter", options=["All"] + stages, key="overview_filter_stage")


    df_view = df.copy()
    if search_q:
        sq = search_q.lower()
        df_view = df_view[df_view.apply(lambda r: sq in str(r.get("lead_id","")).lower() or sq in str(r.get("contact_name","")).lower() or sq in str(r.get("property_address","")).lower() or sq in str(r.get("notes","")).lower(), axis=1)]
    if filter_src and filter_src != "All":
        df_view = df_view[df_view["source"] == filter_src]
    if filter_stage and filter_stage != "All":
        df_view = df_view[df_view["stage"] == filter_stage]


    if df_view.empty:
        st.info("No leads to show.")
        return



# iterate leads (most recent first)


    # iterate leads (most recent first)
    for _, lead in df_view.sort_values("created_at", ascending=False).head(200).iterrows():
        exp_key = f"exp_{lead['lead_id']}"
        with st.expander(f"#{lead['lead_id']} ‚Äî {lead.get('contact_name') or 'No name'} ‚Äî {lead.get('stage')}", expanded=False):
            left, right = st.columns([3,1])
            with left:
                st.write(f"**Source:** {lead.get('source') or ''}  |  **Assigned:** {lead.get('assigned_to') or ''}")
                st.write(f"**Address:** {lead.get('property_address') or ''}")
                st.write(f"**Contact:** {lead.get('contact_name') or ''} / {lead.get('contact_phone') or ''} / {lead.get('contact_email') or ''}")
                st.write(f"**Notes:** {lead.get('notes') or ''}")
                st.write(f"**Created:** {lead.get('created_at')}")
            with right:
                sla_sec, overdue = calculate_remaining_sla(lead.get("sla_entered_at") or lead.get("created_at"), lead.get("sla_hours"))
                if overdue:
                    st.markdown("<div style='color:#dc2626;font-weight:700;'>‚ùó OVERDUE</div>", unsafe_allow_html=True)
                else:
                    try:
                        hours = int(sla_sec // 3600) if sla_sec is not None else 0
                        mins = int((sla_sec % 3600) // 60) if sla_sec is not None else 0
                    except Exception:
                        hours, mins = 0, 0
                    st.markdown(f"<div class='small-muted'>‚è≥ {hours}h {mins}m left</div>", unsafe_allow_html=True)
            # update form
            c1, c2 = st.columns(2)
            with st.form(f"update_{lead['lead_id']}", clear_on_submit=False):
                new_stage = st.selectbox("Status", PIPELINE_STAGES, index=PIPELINE_STAGES.index(lead.get("stage")) if lead.get("stage") in PIPELINE_STAGES else 0, key=f"stage_{lead['lead_id']}")
                new_assigned = st.text_input("Assigned to (username)", value=lead.get("assigned_to") or "", key=f"assigned_{lead['lead_id']}")
                new_est = st.number_input("Estimated value (USD)", value=float(lead.get("estimated_value") or 0.0), min_value=0.0, step=100.0, key=f"estval_{lead['lead_id']}")
                new_cost = st.number_input("Cost to acquire lead (USD)", value=float(lead.get("ad_cost") or 0.0), min_value=0.0, step=1.0, key=f"cost_{lead['lead_id']}")
                new_notes = st.text_area("Notes", value=lead.get("notes") or "", key=f"notes_{lead['lead_id']}")
                submitted = st.form_submit_button("Save changes", key=f"save_changes_{lead['lead_id']}")
                if submitted:
                    try:
                        upsert_lead_record({
                            "lead_id": lead["lead_id"],
                            "stage": new_stage,
                            "assigned_to": new_assigned or None,
                            "estimated_value": new_est,
                            "ad_cost": new_cost,
                            "notes": new_notes
                        }, actor="admin")
                        st.success("Lead updated")
                        st.rerun()
                    except Exception as e:
                        st.error("Failed to update lead: " + str(e))
                        st.write(traceback.format_exc())
            # Technician assignment (outside form)
            st.markdown("### Technician Assignment")
            techs_df = get_technicians_df(active_only=True)
            tech_options = [""] + (techs_df["username"].tolist() if not techs_df.empty else [])
            selected_tech = st.selectbox("Assign Technician (active)", options=tech_options, index=0, key=f"tech_select_{lead['lead_id']}")
            assign_notes = st.text_area("Assignment notes (optional)", value="", key=f"tech_notes_{lead['lead_id']}")
            if st.button(f"Assign Technician to {lead['lead_id']}", key=f"assign_btn_{lead['lead_id']}"):
                if not selected_tech:
                    st.error("Select a technician")
                else:
                    try:
                        create_inspection_assignment(lead_id=lead["lead_id"], technician_username=selected_tech, notes=assign_notes)
                        upsert_lead_record({
                            "lead_id": lead["lead_id"],
                            "inspection_scheduled": True,
                            "stage": "Inspection Scheduled"
                        }, actor="admin")
                        st.success(f"Assigned {selected_tech} to lead {lead['lead_id']}")
                        st.rerun()
                    except Exception as e:
                        st.error("Failed to assign: " + str(e))
    # end for








# ------------------------------------------------------------
# NEXT PAGE STARTS HERE
# ------------------------------------------------------------


def page_lead_capture():
    require_role_access("lead_capture")
    st.markdown("<div class='header'>üìá Lead Capture</div>", unsafe_allow_html=True)
    st.markdown("<em>Create or upsert a lead. All inputs are saved for reporting and CPA calculations.</em>", unsafe_allow_html=True)
    with st.form("lead_capture_form", clear_on_submit=True):
        lead_id = st.text_input("Lead ID", value=f"L{int(datetime.utcnow().timestamp())}")
        source = st.selectbox("Lead Source", ["Google Ads","Organic Search","Referral","Phone","Insurance","Facebook","Instagram","LinkedIn","Other"])
        source_details = st.text_input("Source details (UTM / notes)", placeholder="utm_source=google...")
        contact_name = st.text_input("Contact name")
        contact_phone = st.text_input("Contact phone")
        contact_email = st.text_input("Contact email")
        property_address = st.text_input("Property address")
        damage_type = st.text_input(
            "Job / Service Type",
            placeholder="e.g. Water Damage, HVAC Repair, Security Patrol, IT Support"
        )

        assigned_to = st.text_input("Assigned to (username)")
        estimated_value = st.number_input("Estimated value (USD)", min_value=0.0, value=0.0, step=100.0)
        ad_cost = st.number_input("Cost to acquire lead (USD)", min_value=0.0, value=0.0, step=1.0)
        sla_hours = st.number_input("SLA hours (first response)", min_value=1, value=DEFAULT_SLA_HOURS, step=1)
        notes = st.text_area("Notes")
        submitted = st.form_submit_button("Create / Update Lead")
        
        if submitted:
            try:
                # ==============================
                # PLAN LIMIT ENFORCEMENT (PASTE HERE)
                # ==============================
                plan = get_current_plan()
                limit = PLANS.get(plan, {}).get("max_leads_per_month")

                if limit is not None:
                    current_count = count_leads_this_month()
                    if current_count >= limit:
                        st.error(
                            f"üö´ You‚Äôve reached your monthly lead limit "
                            f"({current_count}/{limit}). Upgrade to add more leads."
                        )
                        return
                # ==============================
                # END PLAN CHECK
                # ==============================

                upsert_lead_record({
                    "lead_id": lead_id.strip(),
                    "created_at": datetime.utcnow(),
                    "source": source,
                    "source_details": source_details,
                    "contact_name": contact_name,
                    "contact_phone": contact_phone,
                    "contact_email": contact_email,
                    "property_address": property_address,
                    "damage_type": damage_type,
                    "assigned_to": assigned_to or None,
                    "estimated_value": float(estimated_value or 0.0),
                    "ad_cost": float(ad_cost or 0.0),
                    "sla_hours": int(sla_hours or DEFAULT_SLA_HOURS),
                    "sla_entered_at": datetime.utcnow(),
                    "notes": notes
                }, actor="admin")

                st.success(f"Lead {lead_id} saved.")
                st.rerun()

            except Exception as e:
                st.error("Failed to save lead: " + str(e))
                st.write(traceback.format_exc())


# Pipeline Board 
# =============================
# PIPELINE BOARD ‚Äî HYBRID VIEW
# Priority Intelligence + Stage Overview
# =============================

def page_pipeline_board():
    require_role_access("pipeline")
    st.markdown("<div class='header'>üìä Pipeline Intelligence Board</div>", unsafe_allow_html=True)
    st.markdown(
        "<em>Operational pipeline combining urgency, value, and stage visibility.</em>",
        unsafe_allow_html=True,
    )

    # ---------- LOAD DATA ----------
    leads_df = get_leads_df()  # must return SLA, stage, score, estimated_value, assigned_to

    st.subheader("üß± Seasonal Damage Type Distribution")
    
    if "created_at" not in leads_df.columns:
        st.warning("‚ö†Ô∏è Seasonal analysis unavailable: missing created_at data, Please add new Leads.")
        return

    leads_df = leads_df.copy()
    leads_df["created_at"] = pd.to_datetime(leads_df["created_at"], errors="coerce")
    leads_df = leads_df.dropna(subset=["created_at"])
    
    if leads_df.empty:
        st.warning("‚ö†Ô∏è No valid dates available for seasonal analysis.")
        return
    
    leads_df["month"] = leads_df["created_at"].dt.month_name()

    
    damage_month = (
        leads_df
        .groupby(["month", "damage_type"])
        .size()
        .reset_index(name="jobs")
    )
    
    fig_damage = px.bar(
        damage_month,
        x="month",
        y="jobs",
        color="damage_type",
        title="Damage Types by Month (Seasonal Impact)"
    )
    
    st.plotly_chart(fig_damage, use_container_width=True)

    if leads_df.empty:
        st.info("No leads in pipeline yet.")
        return

    # ---------- DERIVED METRICS ----------
    now = datetime.utcnow()
    leads_df["sla_remaining_hr"] = (
        leads_df["sla_entered_at"] + pd.to_timedelta(leads_df["sla_hours"], unit="h") - now
    ).dt.total_seconds() / 3600

    leads_df["priority_score"] = (
        (1 - leads_df["sla_remaining_hr"].clip(lower=0) / leads_df["sla_hours"]) * 0.4
        + leads_df["score"].fillna(0) * 0.4
        + (leads_df["estimated_value"].fillna(0) / 20000).clip(0, 1) * 0.2
    )

    # ---------- URGENCY BANDS ----------
    def urgency_label(row):
        if row.sla_remaining_hr <= 6:
            return "üî¥ Critical"
        if row.sla_remaining_hr <= 24:
            return "üü† High"
        if row.sla_remaining_hr <= 48:
            return "üü° Medium"
        return "üü¢ Normal"

    leads_df["urgency"] = leads_df.apply(urgency_label, axis=1)

    # ---------- TOP PRIORITY QUEUE ----------
    st.markdown("## üî• Priority Queue (What needs attention now)")

    priority_df = (
        leads_df.sort_values("priority_score", ascending=False)
        .head(10)
        [[
            "urgency",
            "lead_id",
            "stage",
            "sla_remaining_hr",
            "assigned_to",
            "estimated_value",
            "score",
        ]]
    )

    st.dataframe(
        priority_df.style.format({
            "sla_remaining_hr": "{:.1f}h",
            "estimated_value": "${:,.0f}",
            "score": "{:.2f}",
        }),
        use_container_width=True,
    )

    # ---------- STAGE OVERVIEW ----------
    st.markdown("---")
    st.markdown("## üß≠ Pipeline by Stage")

    stage_summary = (
        leads_df.groupby("stage")
        .agg(
            leads=("lead_id", "count"),
            value=("estimated_value", "sum"),
            avg_score=("score", "mean"),
        )
        .reset_index()
    )

    c1, c2 = st.columns(2)

    with c1:
        st.plotly_chart(
            px.bar(
                stage_summary,
                x="stage",
                y="leads",
                title="Lead Volume by Stage",
            ),
            use_container_width=True,
        )

    with c2:
        st.plotly_chart(
            px.bar(
                stage_summary,
                x="stage",
                y="value",
                title="Pipeline Value by Stage",
            ),
            use_container_width=True,
        )

    # ---------- STAGE DETAIL TABLE ----------
    st.markdown("### üìã Detailed Stage Breakdown")

    selected_stage = st.selectbox(
        "View stage details",
        sorted(leads_df["stage"].unique()),
    )

    stage_df = leads_df[leads_df["stage"] == selected_stage].sort_values(
        "priority_score", ascending=False
    )

    st.dataframe(
        stage_df[[
            "urgency",
            "lead_id",
            "assigned_to",
            "sla_remaining_hr",
            "estimated_value",
            "score",
        ]].style.format({
            "sla_remaining_hr": "{:.1f}h",
            "estimated_value": "${:,.0f}",
            "score": "{:.2f}",
        }),
        use_container_width=True,
    )

    # ---------- EXECUTIVE SUMMARY ----------
    st.markdown("---")
    st.markdown("## üß† Executive Pipeline Insight")

    critical = (leads_df.sla_remaining_hr <= 6).sum()
    total_value = leads_df.estimated_value.sum()
    avg_score = leads_df.score.mean()

    summary = (
        f"The pipeline currently contains **{len(leads_df)} active leads** with a total "
        f"estimated value of **${total_value:,.0f}**. "
        f"There are **{critical} leads** approaching SLA breach, requiring immediate action. "
        f"Overall conversion confidence remains {'strong' if avg_score >= 0.6 else 'moderate' if avg_score >= 0.4 else 'low'}, "
        f"with an average lead quality score of **{avg_score:.2f}**."
    )

    st.info(summary)


# Analytics page (donut + SLA line + overdue table)
def page_analytics():
    require_role_access("analytics")
    st.markdown("<div class='header'>üìà Analytics & SLA</div>", unsafe_allow_html=True)

        # üîí Plan Access Gate
    if not has_access("analytics_intelligence"):
        st.warning("üîí Analytics & Business Intelligence is not available on your current plan.")
        st.info("Upgrade your plan to unlock executive analytics, trends, and insights.")
        return

    
    df = leads_to_df(start, end)

    if df.empty:
        st.info("No leads to analyze.")
        return
    # =========================================================
    # TIME RANGE SELECTION
    # =========================================================
    range_key = st.selectbox(
        "Select Time Window",
        [
            "Today",
            "Last 7 Days",
            "Last 30 Days",
            "Last 90 Days",
            "Last 6 Months",
            "Last 12 Months",
            "Custom"
        ],
        index=2
    )
    
    custom_start = custom_end = None
    if range_key == "Custom":
        c1, c2 = st.columns(2)
        with c1:
            custom_start = st.date_input("Start Date")
        with c2:
            custom_end = st.date_input("End Date")
    
    # =========================================================
    # üß† BUSINESS INTELLIGENCE ENGINE
    # =========================================================
    bi_allowed = has_access("analytics_intelligence")
    
    if bi_allowed:
        intelligence = compute_business_intelligence(
            range_key,
            custom_start,
            custom_end
        )
    else:
        intelligence = {}



    # =========================================================
    # üß† EXECUTIVE SUMMARY (CPA & ROI STYLE)
    # =========================================================
    st.markdown("## üß† Executive Summary")
    
    # --- Top KPI Row (CPA-style cards) ---
    k1, k2, k3, k4 = st.columns(4)
    
    k1.metric(
        "Total Jobs",
        intelligence["volume"]["total_jobs"],
        f"{intelligence['volume']['trend']*100:.1f}%"
    )
    
    k2.metric(
        "Total Revenue",
        f"${intelligence['revenue']['total_revenue']:,.0f}",
        f"{intelligence['revenue']['trend']*100:.1f}%"
    )
    
    k3.metric(
        "Revenue / Job",
        f"${intelligence['efficiency']['revenue_per_job']:,.0f}",
        f"{intelligence['efficiency']['trend']*100:.1f}%"
    )
    
    k4.metric(
        "Business Health",
        f"{intelligence.get('health_score', 0)} / 100"
    )
    
    signals = intelligence.get("strategic_signals", [])
    
    if signals:
        st.markdown("### üö® Strategic Signals")
        cols = st.columns(len(signals))
    
        for col, sig in zip(cols, signals):
            col.markdown(f"""
            <div class="kpi-card">
                <div class="kpi-label">{sig['icon']} {sig['label']}</div>
                <div class="kpi-value cyan">{sig.get('short', 'Active')}</div>
                <div class="kpi-caption">{sig['message']}</div>
            </div>
            """, unsafe_allow_html=True)
    
        
    # =========================================================
    # üß† EXECUTIVE NARRATIVE (Clean, structured)
    # =========================================================
    st.markdown("### üß† Executive Interpretation")
    
    narrative = intelligence.get("executive_narrative", {})
    for line in narrative.get("lines", []):
        confidence = line.get("confidence", 0)
        if confidence >= 80:
            st.success(line["text"])
        elif confidence >= 50:
            st.info(line["text"])
        else:
            st.warning(line["text"])
    

    
        
    health = narrative.get("health_score", 0)
    version = narrative.get("version", "Unknown")
    
    c1, c2 = st.columns(2)
    
    c1.markdown(f"""
    <div class="kpi-card">
        <div class="kpi-label">Narrative Health</div>
        <div class="kpi-value green">{health} / 100</div>
    </div>
    """, unsafe_allow_html=True)
    
        
    risk = intelligence.get("business_risk_score", 0)
    
    st.markdown(f"""
    <div class="kpi-card">
        <div class="kpi-label">‚ö†Ô∏è Business Risk Score</div>
        <div class="kpi-value red">{risk} / 100</div>
    </div>
    """, unsafe_allow_html=True)


    
    st.markdown("<em>Donut of pipeline stages + SLA overdue chart and table</em>", unsafe_allow_html=True)
    #----------- Donut: pipeline stages-----------------
    stage_counts = df["stage"].value_counts().reindex(PIPELINE_STAGES, fill_value=0)
    pie_df = pd.DataFrame({"stage": stage_counts.index, "count": stage_counts.values})
    fig = px.pie(pie_df, names="stage", values="count", hole=0.45, color="stage")
    st.plotly_chart(fig, use_container_width=True)
    st.markdown("---")
    # SLA Overdue time series (last 30 days)
    st.subheader("SLA Overdue (last 30 days)")
    today = datetime.utcnow().date()
    days = [today - timedelta(days=i) for i in range(29, -1, -1)]
    ts = []
    for d in days:
        start_dt = datetime.combine(d, datetime.min.time())
        end_dt = datetime.combine(d, datetime.max.time())
        sub = df[(df["created_at"] >= start_dt) & (df["created_at"] <= end_dt)]
        overdue_cnt = 0
        for _, r in sub.iterrows():
            _, overdue = calculate_remaining_sla(r.get("sla_entered_at") or r.get("created_at"), r.get("sla_hours"))
            if overdue and r.get("stage") not in ("Won","Lost"):
                overdue_cnt += 1
        ts.append({"date": d, "overdue": overdue_cnt})
    ts_df = pd.DataFrame(ts)
    fig2 = px.line(ts_df, x="date", y="overdue", markers=True, title="SLA Overdue Count (30d)")
    st.plotly_chart(fig2, use_container_width=True)
    st.markdown("---")
    st.subheader("Current Overdue Leads")
    overdue_rows = []
    for _, r in df.iterrows():
        _, overdue = calculate_remaining_sla(r.get("sla_entered_at") or r.get("created_at"), r.get("sla_hours"))
        if overdue and r.get("stage") not in ("Won","Lost"):
            overdue_rows.append({"lead_id": r.get("lead_id"), "stage": r.get("stage"), "value": r.get("estimated_value"), "assigned_to": r.get("assigned_to")})
    if overdue_rows:
        st.dataframe(pd.DataFrame(overdue_rows))
    else:
        st.info("No overdue leads currently.")
        
def compute_business_health_score(df, prev_df):
    if df.empty:
        return {"score": 0, "breakdown": {}}

    # ---- Volume Momentum ----
    vol_now = len(df)
    vol_prev = len(prev_df) if not prev_df.empty else vol_now
    volume_score = min(1, vol_now / max(vol_prev, 1))

    # ---- Revenue Quality ----
    rev_now = df["estimated_value"].sum()
    rev_prev = prev_df["estimated_value"].sum() if not prev_df.empty else rev_now
    revenue_score = min(1, rev_now / max(rev_prev, 1))

    # ---- Mix Stability ----
    mix = df.groupby("damage_type")["estimated_value"].sum()
    mix_share = mix / mix.sum()
    mix_score = 1 - mix_share.max()  # higher = more diversified

    # ---- Efficiency ----
    efficiency_now = rev_now / max(vol_now, 1)
    efficiency_prev = (
        rev_prev / max(vol_prev, 1) if not prev_df.empty else efficiency_now
    )
    efficiency_score = min(1, efficiency_now / max(efficiency_prev, 1))

    # ---- Weighted Total ----
    score = round(
        (
            volume_score * 0.25
            + revenue_score * 0.30
            + mix_score * 0.20
            + efficiency_score * 0.25
        ) * 100,
        1,
    )

    return {
        "score": score,
        "breakdown": {
            "Volume Momentum": round(volume_score * 100, 1),
            "Revenue Quality": round(revenue_score * 100, 1),
            "Mix Stability": round(mix_score * 100, 1),
            "Efficiency": round(efficiency_score * 100, 1),
        },
    }

def generate_executive_narrative(data):
    """
    Hardened Executive Narrative Generator with:
    - Confidence scoring
    - Risk flags
    - Narrative health indicator
    - Versioning based on BI data completeness
    Returns a dict with:
      'lines': list of narrative dicts {'text':..., 'confidence':...}
      'risk_flags': list of strings
      'health_score': float (0-100)
      'version': str
    """

    narrative = []
    risk_flags = []
    health_score = 100  # Start at 100 and deduct for missing/incomplete data

    # -----------------------------
    # Safe data extraction
    # -----------------------------
    revenue = data.get("revenue", {})
    volume = data.get("volume", {})
    trends = data.get("trends", {})
    seasonal = data.get("seasonal", {})
    mix = data.get("mix", {})

    # -----------------------------
    # Determine version based on data completeness
    # -----------------------------
    total_keys = ["revenue", "volume", "trends", "seasonal", "mix"]
    available_keys = sum(1 for k in total_keys if data.get(k))
    if available_keys == len(total_keys):
        version = "Full BI"
    elif available_keys >= 3:
        version = "Partial BI"
    else:
        version = "Minimal BI"
        health_score -= 40  # penalty for sparse data

    # -----------------------------
    # Helper function for confidence
    # -----------------------------
    def confidence_for_metric(metric_value, max_score=100):
        """Assign confidence based on data validity"""
        if metric_value is None or metric_value == 0:
            return 40  # low confidence
        elif metric_value < 0:
            return 60
        else:
            return max_score

    # -----------------------------
    # Revenue Narrative
    # -----------------------------
    total_revenue = revenue.get("total", 0)
    revenue_trend = revenue.get("trend", 0)

    rev_conf = confidence_for_metric(total_revenue)
    if total_revenue > 0:
        if revenue_trend > 0:
            narrative.append({
                "text": f"Revenue performance is trending upward, with total revenue reaching ${total_revenue:,.0f}.",
                "confidence": rev_conf
            })
        elif revenue_trend < 0:
            narrative.append({
                "text": f"Revenue shows a slight decline, totaling ${total_revenue:,.0f}, indicating potential market softening.",
                "confidence": rev_conf
            })
            risk_flags.append("Revenue declining")
        else:
            narrative.append({
                "text": f"Revenue remains stable at ${total_revenue:,.0f} for the selected period.",
                "confidence": rev_conf
            })
    else:
        narrative.append({
            "text": "Revenue data is limited for the selected period, suggesting reduced activity or incomplete reporting.",
            "confidence": rev_conf
        })
        health_score -= 20

    # -----------------------------
    # Volume Narrative
    # -----------------------------
    total_jobs = volume.get("total", 0)
    volume_trend = volume.get("trend", 0)
    vol_conf = confidence_for_metric(total_jobs)

    if total_jobs > 0:
        if volume_trend > 0:
            narrative.append({
                "text": f"Job volume increased to {int(total_jobs)} jobs, reflecting growing demand.",
                "confidence": vol_conf
            })
        elif volume_trend < 0:
            narrative.append({
                "text": f"Job volume declined to {int(total_jobs)} jobs, indicating a slowdown in intake.",
                "confidence": vol_conf
            })
            risk_flags.append("Volume declining")
        else:
            narrative.append({
                "text": f"Job volume remained steady at {int(total_jobs)} jobs.",
                "confidence": vol_conf
            })
    else:
        narrative.append({
            "text": "Job volume data is minimal, which may reflect low intake or missing operational records.",
            "confidence": vol_conf
        })
        health_score -= 20

    # -----------------------------
    # Seasonal Insight Narrative
    # -----------------------------
    peak_month = seasonal.get("peak_month")
    low_month = seasonal.get("low_month")
    seasonal_conf = 80 if peak_month or low_month else 40

    if peak_month:
        narrative.append({
            "text": f"Seasonal analysis indicates peak operational activity around {peak_month}, aligning with historical demand patterns.",
            "confidence": 80
        })


def detect_revenue_leakage(data):
    """
    Detects silent revenue decline scenarios.
    Returns a list of leakage signals.
    """

    signals = []

    revenue = data.get("revenue", {})
    volume = data.get("volume", {})
    efficiency = data.get("efficiency", {})

    rev_trend = revenue.get("trend", 0)
    vol_trend = volume.get("trend", 0)
    eff_trend = efficiency.get("trend", 0)

    # Busy but earning less
    if vol_trend >= 0 and rev_trend < 0:
        signals.append(
            "Revenue is declining despite stable or increasing job volume ‚Äî potential pricing or scope leakage."
        )

    # Efficiency erosion
    if eff_trend < -10:
        signals.append(
            "Revenue efficiency is dropping significantly, indicating jobs are generating less value."
        )

    return signals

def is_executive_insight_ready(intelligence):
    """
    Determines whether there is sufficient data
    to display Executive Intelligence insights.
    """

    narrative = intelligence.get("executive_narrative", {})
    narrative_lines = narrative.get("lines", [])

    leakage = intelligence.get("revenue_leakage", [])
    alerts = intelligence.get("strategic_alerts", [])
    changes = intelligence.get("change_explanations", [])

    return any([
        len(narrative_lines) > 0,
        len(leakage) > 0,
        len(alerts) > 0,
        len(changes) > 0,
    ])



def generate_strategic_signals(data):
    """
    Generates high-level strategic alerts.
    Returns list of dicts: {icon, label, message}
    """

    alerts = []

    revenue = data.get("revenue", {})
    volume = data.get("volume", {})
    trends = data.get("trends", {})

    rev_trend = revenue.get("trend", 0)
    vol_trend = volume.get("trend", 0)
    momentum = trends.get("momentum")

    if rev_trend < 0 and vol_trend < 0:
        alerts.append({
            "icon": "‚ö†Ô∏è",
            "label": "Contraction Risk",
            "message": "Both revenue and job volume are declining."
        })

    if rev_trend > 10 and momentum == "positive":
        alerts.append({
            "icon": "üî•",
            "label": "Growth Window",
            "message": "Strong revenue growth supported by positive momentum."
        })

    if abs(rev_trend) < 3 and abs(vol_trend) < 3:
        alerts.append({
            "icon": "üßä",
            "label": "Stagnation",
            "message": "Business activity appears flat with limited movement."
        })

    return alerts

def compute_business_risk_score(data):
    """
    Computes an overall business risk score (0‚Äì100).
    Higher = more risk.
    """

    score = 0

    revenue = data.get("revenue", {})
    volume = data.get("volume", {})
    efficiency = data.get("efficiency", {})
    leakage = data.get("revenue_leakage", [])

    # Revenue decline
    if revenue.get("trend", 0) < 0:
        score += 25

    # Volume decline
    if volume.get("trend", 0) < 0:
        score += 20

    # Efficiency erosion
    if efficiency.get("trend", 0) < -10:
        score += 20

    # Silent leakage signals
    score += len(leakage) * 15

    return min(score, 100)


def explain_what_changed(current, previous):
    """
    Compares two BI snapshots and explains key changes.
    Returns human-readable change explanations.
    """

    explanations = []

    def compare(metric, label):
        cur = current.get(metric, {}).get("total", 0)
        prev = previous.get(metric, {}).get("total", 0)

        if prev == 0:
            return

        change_pct = ((cur - prev) / prev) * 100

        if abs(change_pct) >= 10:
            direction = "increased" if change_pct > 0 else "decreased"
            explanations.append(
                f"{label} {direction} by {abs(change_pct):.1f}% compared to the previous period."
            )

    compare("revenue", "Revenue")
    compare("volume", "Job volume")

    if not explanations:
        explanations.append(
            "No significant changes detected compared to the previous period."
        )

    return explanations

def page_executive_intelligence():
    require_role_access("intelligence")
    st.title("üìä Executive Intelligence Overview")

    intelligence = compute_business_intelligence(
        st.session_state.get("range_key"),
        st.session_state.get("custom_start"),
        st.session_state.get("custom_end")
    )

    # Risk at the top
    st.metric(
        "Overall Business Risk",
        f"{intelligence.get('business_risk_score', 0)} / 100"
    )

    # Strategic Signals
    st.markdown("## üö® Strategic Signals")
    for sig in intelligence.get("strategic_signals", []):
        st.write(f"{sig['icon']} **{sig['label']}** ‚Äî {sig['message']}")

    
    # üß† Executive Summary
    bi_allowed = has_access("analytics_intelligence")
    
    st.markdown("## üß† Executive Summary")
    narrative = intelligence.get("executive_narrative", {})
    for line in narrative.get("lines", []):
        st.write("‚Ä¢", line["text"])
    
    # Display Narrative Health metric once
    if bi_allowed:
        st.metric("Business Risk Score", f"{risk} / 100")
    else:
        st.metric("Business Risk Score", "üîí Locked")
        st.caption("Upgrade to unlock Business Intelligence")
    
        
    # Revenue Leakage

    st.markdown("## üí∏ Revenue Leakage Risks")
    for issue in intelligence.get("revenue_leakage", []):
        st.write("‚Ä¢", issue)

    # Change Analysis
    st.markdown("## üîÑ Period-over-Period Changes")
    for change in intelligence.get("what_changed", []):
        st.write("‚Ä¢", change)
    # ---------------------------------
    # Executive Data Readiness Gate
    # ---------------------------------
    if not is_executive_insight_ready(intelligence):
        st.markdown("## üß† Executive Intelligence")
    
        st.info(
            "Not enough data is available yet to generate reliable executive-level insights.\n\n"
            "This system only surfaces intelligence when confidence thresholds are met ‚Äî "
            "no speculation, no guesswork."
        )
    
        st.markdown("### What‚Äôs missing")
        missing = []
    
        if not intelligence.get("executive_narrative", {}).get("lines"):
            missing.append("‚Ä¢ Trend-based executive narrative")
    
        if not intelligence.get("revenue_leakage"):
            missing.append("‚Ä¢ Revenue leakage indicators")
    
        if not intelligence.get("strategic_alerts"):
            missing.append("‚Ä¢ Strategic signals (growth, contraction, stagnation)")
    
        if not intelligence.get("change_explanations"):
            missing.append("‚Ä¢ Period-over-period change explanations")
    
        for item in missing:
            st.write(item)
    
        st.markdown("### What to do next")
        st.write("‚Ä¢ Continue logging jobs and revenue")
        st.write("‚Ä¢ Expand the selected date range")
        st.write("‚Ä¢ Ensure historical comparison periods exist")
    
        st.caption("Executive Intelligence activates automatically once sufficient data is available.")
        return


def generate_executive_narrative(data):
    narrative = []
    risk_flags = []

    volume = data.get("volume", {})
    revenue = data.get("revenue", {})
    efficiency = data.get("efficiency", {})

    # build narrative lines as dicts
    narrative.append({
        "text": "...",
        "confidence": 85
    })

    health_score = 82

    return {
        "lines": narrative,
        "risk_flags": risk_flags,
        "health_score": health_score,
        "version": "G-hardened-v1"
    }


def page_business_intelligence():
    require_role_access("business_intelligence")
    # -----------------------------
    # üß† EXECUTIVE BUSINESS HEALTH
    # -----------------------------
    st.markdown("## üß† Business Health Score")
    
    health = data.get("health")
    
    if health:
        st.metric(
            "Overall Business Health",
            f"{health['score']} / 100",
            help="Composite score of volume, revenue quality, mix stability, and efficiency"
        )
    
        cols = st.columns(4)
        for i, (k, v) in enumerate(health.get("breakdown", {}).items()):
            cols[i].metric(k, f"{v}%")
    else:
        st.info("Business health score not available for this period.")
    
    # -----------------------------
    # üß† HEALTH SCORE INTERPRETATION
    # -----------------------------
    if health and "score" in health:
        if health["score"] >= 75:
            st.success("üü¢ Business is strong and scaling efficiently.")
        elif health["score"] >= 55:
            st.warning("üü° Business is stable but showing early pressure.")
        else:
            st.error("üî¥ Business health is deteriorating. Immediate review recommended.")

    
    st.markdown("### üîÑ Period-over-Period Change")
    
    comp = data["comparison"]
    
    c1, c2, c3 = st.columns(3)
    
    def fmt(p):
        if p is None:
            return "N/A"
        return f"{p*100:+.1f}%"
    
    c1.metric("Jobs vs Last Period", fmt(comp["jobs_change_pct"]))
    c2.metric("Revenue vs Last Period", fmt(comp["revenue_change_pct"]))
    c3.metric("Avg Revenue / Job", fmt(comp["avg_rev_change_pct"]))
    
    
    st.markdown("### üå¶Ô∏è Seasonal Context")
    
    if data.get("seasonal"):
        for s in data["seasonal"]:
            st.warning(s)
    else:
        st.success("üü¢ Performance is within expected seasonal range")
    

    # -----------------------------
    # TIME WINDOW CONTROLS
    # -----------------------------
    col1, col2 = st.columns([2, 3])

    with col1:
        range_key = st.selectbox(
            "Time Range",
            ["daily", "weekly", "30d", "90d", "6m", "12m", "custom"],
            index=2
        )

    with col2:
        custom_start, custom_end = None, None
        if range_key == "custom":
            custom_start = st.date_input("Start date")
            custom_end = st.date_input("End date")

    data = compute_business_intelligence(
        range_key,
        custom_start,
        custom_end
    )

    df = data.get("raw_df")

    if df is None or df.empty:
        st.warning("No jobs found for this period.")
        return

    # -----------------------------
    # JOB VOLUME & MIX
    # -----------------------------
    st.markdown("### üìä Job Volume & Mix")

    v = data.get("volume", {})

    c1, c2, c3 = st.columns(3)
    c1.metric("Total Jobs", v.get("total_jobs", 0))
    c2.metric("Jobs / Day", v.get("jobs_per_day", 0))
    c3.metric("Lead Sources", len(v.get("lead_sources", {})))

    if v.get("job_types"):
        st.plotly_chart(
            px.bar(
                x=list(v["job_types"].keys()),
                y=list(v["job_types"].values()),
                labels={"x": "Job Type", "y": "Count"},
                title="Jobs by Type"
            ),
            use_container_width=True
        )

    # -----------------------------
    # REVENUE INTELLIGENCE
    # -----------------------------
    st.markdown("### üí∞ Revenue Intelligence")

    r = data.get("revenue", {})

    c1, c2, c3 = st.columns(3)
    c1.metric("Total Revenue", f"${r.get('total_revenue', 0):,.0f}")
    c2.metric("Avg / Job", f"${r.get('avg_revenue_per_job', 0):,.0f}")
    c3.metric(
        "Revenue Risk",
        f"{int(r.get('revenue_concentration', 0) * 100)}% Top Dependency"
    )

    if r.get("revenue_by_job_type"):
        st.plotly_chart(
            px.pie(
                names=list(r["revenue_by_job_type"].keys()),
                values=list(r["revenue_by_job_type"].values()),
                title="Revenue Share by Job Type"
            ),
            use_container_width=True
        )
    # -----------------------------
    # JOB TYPE INTELLIGENCE
    # -----------------------------
    st.markdown("### üß© Job Type Intelligence")

    jt = data["job_types"]
    
    if jt:
        st.dataframe(
            jt["table"][
                ["damage_type", "jobs", "job_share", "revenue", "revenue_share", "avg_revenue"]
            ],
            use_container_width=True
        )
    
        c1, c2 = st.columns(2)
    
        with c1:
            st.plotly_chart(
                px.pie(
                    jt["table"],
                    names="damage_type",
                    values="jobs",
                    title="Job Volume Distribution"
                ),
                use_container_width=True
            )
    
        with c2:
            st.plotly_chart(
                px.pie(
                    jt["table"],
                    names="damage_type",
                    values="revenue",
                    title="Revenue Distribution by Job Type"
                ),
                use_container_width=True
            )
    
        if jt["insights"]:
            st.markdown("#### ‚ö†Ô∏è Job Type Signals")
            for i in jt["insights"]:
                st.warning(i)
    else:
        st.info("No job type data available for this period.")


    # -----------------------------
    # EFFICIENCY & LEVERAGE
    # -----------------------------
    st.markdown("### ‚öôÔ∏è Efficiency & Leverage")

    ineff = data.get("efficiency", {}).get("high_volume_low_value_jobs", [])

    if ineff:
        st.warning(
            "‚ö†Ô∏è High-effort, low-return job types detected: "
            + ", ".join(ineff)
        )
    else:
        st.success("No major efficiency leaks detected in this period.")

    # -----------------------------
    # EXECUTIVE SIGNALS
    # -----------------------------
    st.markdown("### üß† Executive Signals")

    signals = data.get("signals", [])

    if not signals:
        st.info("No abnormal signals detected. Business activity is within normal range.")
    else:
        for s in signals:
            st.error(s)

    # -----------------------------
    # RAW DATA
    # -----------------------------
    with st.expander("üìÑ View Underlying Jobs Data"):
        st.dataframe(df, use_container_width=True)

    



    
    STATUS_COLORS = {
    "live": "green",
    "recent": "orange",
    "offline": "red",
}
def page_technician_map_tracking():
    st.markdown("## üó∫Ô∏è Technician Live Map")

    df = get_latest_location_pings()

    if df.empty:
        st.warning("No technician GPS data available.")
        return

    # Convert timestamp safely
    df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce")

    def classify_status(ts):
        if pd.isna(ts):
            return "offline"
        mins = (datetime.utcnow() - ts).total_seconds() / 60
        if mins <= 10:
            return "active"
        elif mins <= 30:
            return "idle"
        return "offline"

    df["status"] = df["timestamp"].apply(classify_status)

    status_color = {
        "active": "green",
        "idle": "orange",
        "offline": "red"
    }

    # Center map
    center_lat = df["latitude"].mean()
    center_lon = df["longitude"].mean()

    m = folium.Map(
        location=[center_lat, center_lon],
        zoom_start=11,
        tiles="OpenStreetMap"
    )

    for _, r in df.iterrows():
        folium.CircleMarker(
            location=[r["latitude"], r["longitude"]],
            radius=8,
            color=status_color[r["status"]],
            fill=True,
            fill_opacity=0.85,
            popup=f"""
            <b>Technician:</b> {r['tech_username']}<br>
            <b>Status:</b> {r['status']}<br>
            <b>Last Ping:</b> {r['timestamp']}
            """
        ).add_to(m)

    st_folium(m, height=600, use_container_width=True)
    st.success("Technician map page loaded")


def page_technician_mobile():
    require_role_access("technicians")
    st.markdown("## üìç Technician Live Location")

    tech_username = st.text_input("Your technician username")

    if not tech_username:
        st.info("Enter your username to begin tracking")
        return

    st.markdown(
        """
        <script>
        navigator.geolocation.getCurrentPosition(
            (pos) => {
                const lat = pos.coords.latitude;
                const lon = pos.coords.longitude;
                const acc = pos.coords.accuracy;

                fetch("/?_save_location=1", {
                    method: "POST",
                    headers: {"Content-Type": "application/json"},
                    body: JSON.stringify({
                        username: "%s",
                        lat: lat,
                        lon: lon,
                        accuracy: acc
                    })
                });
            }
        );
        </script>
        """ % tech_username,
        unsafe_allow_html=True
    )

    st.success("üì° Location sent (refreshes every time page loads)")


# CPA & ROI page
def page_cpa_roi():
    require_role_access("analytics")
    st.markdown("<div class='header'>üí∞ CPA & ROI</div>", unsafe_allow_html=True)
    st.markdown("<em>Total Marketing Spend vs Conversions and ROI calculations.</em>", unsafe_allow_html=True)
    df = leads_df.copy()
    if df.empty:
        st.info("No leads")
        return
    total_spend = float(df["ad_cost"].sum())
    won_df = df[df["stage"] == "Won"]
    conversions = len(won_df)
    cpa = (total_spend / conversions) if conversions else 0.0
    revenue = float(won_df["estimated_value"].sum())
    roi = revenue - total_spend
    roi_pct = (roi / total_spend * 100) if total_spend else 0.0
    c1, c2, c3, c4 = st.columns(4)
    c1.markdown(f"<div class='kpi-card'><div class='kpi-title'>Total Marketing Spend</div><div class='kpi-number' style='color:{KPI_COLORS[0]}'>${total_spend:,.2f}</div></div>", unsafe_allow_html=True)
    c2.markdown(f"<div class='kpi-card'><div class='kpi-title'>Conversions (Won)</div><div class='kpi-number' style='color:{KPI_COLORS[1]}'>{conversions}</div></div>", unsafe_allow_html=True)
    c3.markdown(f"<div class='kpi-card'><div class='kpi-title'>CPA</div><div class='kpi-number' style='color:{KPI_COLORS[3]}'>${cpa:,.2f}</div></div>", unsafe_allow_html=True)
    c4.markdown(f"<div class='kpi-card'><div class='kpi-title'>ROI</div><div class='kpi-number' style='color:{KPI_COLORS[6]}'>${roi:,.2f} ({roi_pct:.1f}%)</div></div>", unsafe_allow_html=True)
    st.markdown("---")
    # chart: spend vs conversions by source
    agg = df.groupby("source").agg(total_spend=("ad_cost","sum"), conversions=("stage", lambda s: (s=="Won").sum())).reset_index()
    if not agg.empty:
        fig = px.bar(agg, x="source", y=["total_spend","conversions"], barmode="group", title="Total Spend vs Conversions by Source")
        st.plotly_chart(fig, use_container_width=True)


# ML internal page
def page_ml_internal():
    st.markdown("<div class='header'>üß† Internal ML ‚Äî Lead Scoring</div>", unsafe_allow_html=True)
    st.markdown("<em>Model runs internally and writes score back to leads. No user tuning exposed.</em>", unsafe_allow_html=True)
    if st.button("Train model (internal)"):
        with st.spinner("Training..."):
            try:
                acc, msg = train_internal_model()
                if acc is None:
                    st.error(f"Training aborted: {msg}")
                else:
                    st.success(f"Model trained (accuracy approx): {acc:.3f}")
            except Exception as e:
                st.error("Training failed: " + str(e))
                st.write(traceback.format_exc())
    model, cols = load_internal_model()
    if model:
        st.success("Model available (internal)")
        if st.button("Score all leads and persist scores"):
            df = leads_to_df()
            scored = score_dataframe(df.copy(), model, cols)
            s = get_session()
            try:
                for _, r in scored.iterrows():
                    lead = s.query(Lead).filter(Lead.lead_id == r["lead_id"]).first()
                    if lead:
                        lead.score = float(r["score"])
                        s.add(lead)
                s.commit()
                st.success("Scores persisted to DB")
            except Exception as e:
                s.rollback()
                st.error("Failed to persist scores: " + str(e))
            finally:
                s.close()
        if st.checkbox("Preview top scored leads"):
            df = leads_to_df()
            scored = score_dataframe(df.copy(), model, cols).sort_values("score", ascending=False).head(20)
            st.dataframe(scored[["lead_id","source","stage","estimated_value","ad_cost","score"]])


# ---------- BEGIN BLOCK G: AI RECOMMENDATIONS PAGE ----------
def page_ai_recommendations():
    require_role_access("business_intelligence")
    """AI Recommendations ‚Äî cleaned, safe, and optimized."""
    import plotly.express as px
    from sqlalchemy import func


    st.markdown("<div class='header'>ü§ñ AI Recommendations</div>", unsafe_allow_html=True)
    st.markdown("<em>Heuristic recommendations and quick diagnostics for the pipeline.</em>", unsafe_allow_html=True)
        # üîí Plan Access Gate
    if not has_access("analytics_intelligence"):
        st.warning("üîí Analytics & Business Intelligence is not available on your current plan.")
        st.info("Upgrade your plan to unlock executive analytics, trends, and insights.")
        return



    # Load leads defensively
    try:
        df = leads_to_df()
    except Exception as e:
        st.error(f"Failed to load leads: {e}")
        df = pd.DataFrame()


    if df.empty:
        st.info("No leads to analyze.")
        return


    # 1) Top overdue leads
    st.subheader("Top Overdue Leads")
    overdue_list = []
    for _, r in df.iterrows():
        rem_s, overdue_flag = calculate_remaining_sla(r.get("sla_entered_at") or r.get("created_at"), r.get("sla_hours"))
        if overdue_flag and r.get("stage") not in ("Won", "Lost"):
            overdue_list.append({
                "lead_id": r["lead_id"],
                "stage": r.get("stage"),
                "assigned_to": r.get("assigned_to"),
                "value": r.get("estimated_value") or 0.0,
                "overdue_seconds": rem_s
            })
    if overdue_list:
        over_df = pd.DataFrame(overdue_list).sort_values("value", ascending=False)
        # keep columns unique and friendly
        over_df = over_df.rename(columns={"lead_id": "Lead ID", "stage": "Stage", "assigned_to": "Assigned To", "value": "Est. Value", "overdue_seconds": "Overdue Seconds"})
        st.table(over_df[["Lead ID", "Stage", "Assigned To", "Est. Value"]].head(10))
    else:
        st.info("No overdue leads.")


    st.markdown("---")


    # 2) Pipeline Bottlenecks (stage counts)
    st.subheader("Pipeline Bottlenecks")
    try:
        stages = PIPELINE_STAGES
    except Exception:
        stages = ["New", "Contacted", "Inspection Scheduled", "Inspection", "Estimate Sent", "Won", "Lost"]


    stage_counts = df["stage"].value_counts().reindex(stages, fill_value=0)
    stage_df = stage_counts.reset_index()
    stage_df.columns = ["Stage", "Count"]        # ensure unique column names
    st.table(stage_df.head(10))


    # Small horizontal bar chart (plotly)
    try:
        fig = px.bar(stage_df, x="Count", y="Stage", orientation="h", title="Leads by Stage", height=300)
        fig.update_layout(margin=dict(l=0, r=0, t=30, b=0))
        st.plotly_chart(fig, use_container_width=True)
    except Exception:
        pass


    st.markdown("---")
 

    # 3) Technician workload (from assignments)
    st.subheader("Technician Workload (Assigned Inspections)")
    try:
        s = get_session()
        try:
            rows = s.query(InspectionAssignment.technician_username, func.count(InspectionAssignment.id)).group_by(InspectionAssignment.technician_username).all()
            if rows:
                tw = pd.DataFrame(rows, columns=["Technician", "Assigned Inspections"])
                tw = tw.sort_values("Assigned Inspections", ascending=False).reset_index(drop=True)
                st.table(tw)
            else:
                st.info("No assignments yet.")
        finally:
            s.close()
    except Exception as e:
        st.error("Failed to load assignments: " + str(e))


    st.markdown("---")


    # 4) Suggestions (simple heuristics)
    st.subheader("Suggested Actions")
    suggestions = []


    # Scheduled but unassigned
    scheduled = df[(df["inspection_scheduled"] == True)]
    scheduled_unassigned = scheduled[(scheduled["assigned_to"].isnull()) | (scheduled["assigned_to"] == "")]
    for _, r in scheduled_unassigned.iterrows():
        suggestions.append(f"Lead {r['lead_id']} is scheduled for inspection but has no assigned technician ‚Äî assign ASAP.")


    # High-value not contacted
    high_uncontacted = df[(df["estimated_value"] >= 5000) & (df["contacted"] == False)]
    for _, r in high_uncontacted.iterrows():
        suggestions.append(f"High-value lead {r['lead_id']} (${int(r['estimated_value']):,}) not contacted ‚Äî prioritize contact within SLA.")


    # Many leads stuck in same stage
    bottleneck = stage_df.sort_values("Count", ascending=False).iloc[0] if not stage_df.empty else None
    if bottleneck is not None and bottleneck["Count"] > max(5, len(df) * 0.2):
        suggestions.append(f"Stage '{bottleneck['Stage']}' has {int(bottleneck['Count'])} leads ‚Äî check for process blockers in this stage.")


    # Show suggestions
    if suggestions:
        for sgt in suggestions[:15]:
            st.markdown(f"- {sgt}")
    else:
        st.markdown("No immediate suggestions. Pipeline looks healthy.")


    st.markdown("---")


    # 5) Quick export of problematic leads (CSV)
    st.subheader("Export: Problem Leads")
    try:
        problem_df = over_df if (len(over_df) > 0) else pd.DataFrame()
        if not problem_df.empty:
            csv = problem_df.to_csv(index=False)
            st.download_button("Download overdue leads (CSV)", data=csv, file_name="overdue_leads.csv", mime="text/csv")
        else:
            st.info("No overdue leads to export.")
    except Exception:
        pass


    # End of page
from passlib.context import CryptContext

pwd_context = CryptContext(
    schemes=["bcrypt"],
    deprecated="auto"
)

def hash_password(password: str) -> str:
    return pwd_context.hash(password)

def verify_password(plain_password: str, hashed_password: str) -> bool:
    return pwd_context.verify(plain_password, hashed_password)

# ----------------------
# WORDPRESS AUTH
# ----------------------

def wp_auth_bridge():
    st.title("Authenticating...")

    token = st.query_params.get("token")
    if not token:
        st.error("Missing authentication token")
        st.stop()

    payload = decode_wp_token(token)
    if not payload:
        st.error("Invalid or expired token")
        st.stop()

    email = payload.get("email")
    name = payload.get("name", "")
    role = payload.get("role", "Viewer")

    if not email:
        st.error("Invalid token payload")
        st.stop()

    with SessionLocal() as s:
        user = s.query(User).filter(User.email == email).first()

        # ----------------------------
        # CREATE USER IF NOT EXISTS
        # ----------------------------
        if not user:
            user = User(
                email=email.lower(),
                username=email.split("@")[0],
                full_name=name,
                role=role,
                plan="starter",
                subscription_status="trial",
                trial_ends_at=datetime.utcnow() + timedelta(days=14),
                email_verified=True,     # ‚úÖ WP auth = verified
                is_active=True,
            )
            s.add(user)
            s.commit()

        # ----------------------------
        # STEP H ‚Äî ENFORCEMENT
        # ----------------------------
        if not user.is_active:
            st.error("Your account has been deactivated.")
            st.stop()

        if not user.email_verified:
            st.error("Please verify your email address before accessing the app.")
            st.stop()

        # ----------------------------
        # SESSION BINDING (CRITICAL)
        # ----------------------------
        st.session_state["user_id"] = user.id
        st.session_state["role"] = user.role
        st.session_state["plan"] = user.plan

    st.success("Authentication successful")
    st.query_params.clear()
    st.rerun()


    # Authenticate Streamlit session
    st.session_state["authenticated"] = True
    st.session_state["user_email"] = email
    st.session_state["role"] = role
    st.session_state["plan"] = user.plan

    st.success("Login successful")
    st.rerun()

def request_password_reset(email: str):
    with SessionLocal() as s:
        user = s.query(User).filter(User.email == email.lower()).first()

        if not user:
            return  # Silent fail for security

        token = generate_password_reset_token()

        user.password_reset_token = token
        user.password_reset_expires_at = datetime.utcnow() + timedelta(hours=1)

        s.commit()

        reset_link = f"{FRONTEND_URL}/reset-password?token={token}"

        send_password_reset_email(user.email, reset_link)

def send_password_reset_email(email: str, reset_link: str):
    # TEMP: console output for testing
    print("PASSWORD RESET LINK:", reset_link)

    # Later you can wire:
    # SendGrid / Mailgun / SMTP / SES

def reset_password_with_token(token: str, new_password_hash: str):
    with SessionLocal() as s:
        user = s.query(User).filter(
            User.password_reset_token == token,
            User.password_reset_expires_at > datetime.utcnow()
        ).first()

        if not user:
            raise ValueError("Invalid or expired reset token")

        user.password_hash = new_password_hash
        user.password_reset_token = None
        user.password_reset_expires_at = None

        s.commit()
#--------------------OPTIONAL PAGE RESET FOR STREAMLIT---------------
def page_reset_password():
    st.markdown("## üîê Reset Password")

    token = st.query_params.get("token")
    if not token:
        st.error("Missing reset token")
        return

    new_password = st.text_input("New Password", type="password")

    if st.button("Reset Password"):
        try:
            hashed = hash_password(new_password)  # your existing hasher
            reset_password_with_token(token, hashed)
            st.success("Password reset successful")
            st.query_params.clear()
        except Exception as e:
            st.error(str(e))

request_password_reset("your@email.com")


def authenticate_user(email: str, password: str):
    with SessionLocal() as s:
        user = s.query(User).filter(
            User.email == email.lower(),
            User.is_active == True
        ).first()

        if not user:
            return None

        if not user.password_hash:
            return None

        if not verify_password(password, user.password_hash):
            return None

        user.last_login_at = datetime.utcnow()
        s.commit()

        return user

def page_login():
    st.markdown("## üîê Login")

    email = st.text_input("Email")
    password = st.text_input("Password", type="password")

    if st.button("Login"):
        user = authenticate_user(email, password)
        if not user:
            st.error("Invalid credentials or inactive account")
        else:
            set_logged_in_user(user)
            st.success("Logged in successfully")
            st.rerun()

# -------------------------
# Settings Page
# -------------------------
def page_settings():
    require_role_access("settings")

    st.markdown(
        "<div class='header'>‚öôÔ∏è Settings & User Management</div>",
        unsafe_allow_html=True
    )
    st.markdown(
        "<em>Add team users, invite users, manage roles and technicians.</em>",
        unsafe_allow_html=True
    )

    # ======================================================
    # üìß INVITE USER (EMAIL + EXPIRY TOKEN)
    # ======================================================
    st.markdown("### üìß Invite User")

    with st.form("invite_user_form"):
        invite_email = st.text_input("Email (required)")
        invite_role = st.selectbox(
            "Role",
            ["Admin", "Manager", "Staff"],
            key="invite_role"
        )

        submitted_invite = st.form_submit_button("Send Invite")

        if submitted_invite:
            if not invite_email:
                st.error("Email is required")
                st.stop()

            if not is_valid_email(invite_email):
                st.error("Enter a valid email address (example@domain.com)")
                st.stop()

            with SessionLocal() as s:
                exists = s.query(User).filter(User.email == invite_email.lower()).first()

                if exists:
                    st.error("User already exists")
                else:
                    # üîê CREATE EXPIRING INVITE TOKEN
                    token = generate_activation_token()

                    user = User(
                        username=invite_email,
                        email=invite_email.lower(),
                        role=invite_role,
                        plan="starter",
                        subscription_status="trial",
                        trial_ends_at=datetime.utcnow() + timedelta(days=14),
                        activation_token=token,
                        activation_expires_at=datetime.utcnow() + timedelta(hours=48),
                        is_active=False,
                    )

                    s.add(user)
                    s.commit()

                    invite_link = f"{FRONTEND_URL}/activate?token={token}"

                    # ‚úÖ For now (testing / copy-paste)
                    st.write("Invite link:", invite_link)

                    st.success("Invite created successfully")
                    st.rerun()

    st.markdown("---")

    # ======================================================
    # ‚ûï ADD USER (INTERNAL / ADMIN ‚Äì IMMEDIATE ACCESS)
    # ======================================================
    st.markdown("### ‚ûï Add User")

    with st.form("create_user_form"):
        email = st.text_input("Email (required)")
        username = st.text_input("Username (optional ‚Äî defaults to email)")
        full_name = st.text_input("Full Name")
        role = st.selectbox(
            "Role",
            ["Admin", "Manager", "Staff"],
            key="create_role"
        )

        submitted_create = st.form_submit_button("Create User")

        if submitted_create:
            if not email:
                st.error("Email is required")
                st.stop()

            if not is_valid_email(email):
                st.error("Please enter a valid email address")
                st.stop()

            try:
                add_user(
                    email=email.lower(),
                    username=username.strip() if username else email.lower(),
                    full_name=full_name.strip(),
                    role=role,
                    is_active=True,
                    email_verified=True,
                )
                st.success("User created successfully")
                st.rerun()

            except Exception as e:
                st.error(f"Failed to create user: {e}")

    # ======================================================
    # üë• USERS TABLE
    # ======================================================
    st.markdown("### üë• Existing Users")

    users_df = get_users_df()
    if users_df.empty:
        st.info("No users yet.")
    else:
        st.dataframe(users_df, use_container_width=True)

    st.markdown("---")

    # ======================================================
    # üë∑ TECHNICIAN MANAGEMENT
    # ======================================================
    st.markdown("## üë∑ Technician Management")

    with st.expander("‚ûï Add Technician", expanded=False):
        col1, col2 = st.columns(2)

        with col1:
            tech_username = st.text_input("Username (unique)")
            tech_name = st.text_input("Full Name")
            tech_phone = st.text_input("Phone Number")

        with col2:
            tech_role = st.selectbox(
                "Specialization",
                ["Estimator", "Technician", "Inspector", "Adjuster", "Other"]
            )
            tech_active = st.checkbox("Active", value=True)

        if st.button("Save Technician"):
            if not tech_username:
                st.error("Username is required")
            else:
                try:
                    add_technician(
                        tech_username.strip(),
                        full_name=tech_name.strip(),
                        phone=tech_phone.strip(),
                        specialization=tech_role,
                        active=tech_active,
                    )
                    st.success("Technician saved")
                    st.rerun()
                except Exception as e:
                    st.error(f"Failed to save technician: {e}")

    st.markdown("### üìã Existing Technicians")

    tech_df = get_technicians_df(active_only=False)
    if tech_df.empty:
        st.info("No technicians added yet.")
    else:
        for _, row in tech_df.iterrows():
            c1, c2, c3 = st.columns([3, 2, 2])

            with c1:
                st.write(f"üë∑ **{row['full_name']}** (`{row['username']}`)")

            with c2:
                new_status = st.selectbox(
                    "Status",
                    ["available", "assigned", "enroute", "onsite", "completed"],
                    index=[
                        "available",
                        "assigned",
                        "enroute",
                        "onsite",
                        "completed",
                    ].index(row.get("status", "available")),
                    key=f"status_{row['username']}",
                )

            with c3:
                if st.button("Update", key=f"btn_{row['username']}"):
                    update_technician_status(row["username"], new_status)
                    st.success("Status updated")
                    st.rerun()


def page_technician_mobile():
    st.markdown("## üì± Technician Mobile")

    techs = get_technicians_df(active_only=True)
    if techs.empty:
        st.warning("No active technicians found.")
        return

    tech = st.selectbox("Select Technician", techs["username"].tolist())

    st.markdown("### üßæ My Tasks")
    tasks = get_tasks_for_user(tech)
    if tasks.empty:
        st.info("No task assigned to a Technician yet! To assign job task to a technician, go to: SETTINGS at the Navigation Menu, then click on the TECHNICIAN MANAGEMENT")
    else:
        for _, t in tasks.iterrows():
            st.checkbox(
                f"{t['title']} (Lead: {t['lead_id']})",
                value=(t["status"] == "done"),
                key=f"task_{t['id']}"
            )

    st.markdown("### üìç Send Location Ping")
    lat = st.number_input("Latitude", format="%.6f")
    lon = st.number_input("Longitude", format="%.6f")

    if st.button("Send Location Ping"):
        persist_location_ping(tech, lat, lon)
        st.success("Location sent")

    # ---------- BEGIN BLOCK D: SETTINGS UI - TECHNICIANS MANAGEMENT ----------
    st.markdown("---")
    st.subheader("Technicians (Field Users)")
    tech_df = get_technicians_df(active_only=False)
    with st.form("add_technician_form"):
        t_uname = st.text_input("Technician username (unique)")
        t_name = st.text_input("Full name")
        t_phone = st.text_input("Phone")
        t_role_sel = st.selectbox("Specialization", ["Tech", "Estimator", "Adjuster", "Driver"], index=0)
        t_active = st.checkbox("Active", value=True)
        if st.form_submit_button("Add / Update Technician"):
            if not t_uname:
                st.error("Technician username required")
            else:
                try:
                    add_technician(
                    username=tech_username.strip(),
                    full_name=tech_name.strip(),
                    phone=tech_phone.strip(),
                    specialization=tech_role,
                    active=tech_active
                )

                except Exception as e:
                    st.error("Failed to save technician: " + str(e))
    if tech_df is not None and not tech_df.empty:
        st.dataframe(tech_df)
    else:
        st.info("No technicians yet.")


# ---------- END BLOCK D ----------


    st.subheader("Priority weight tuning (internal)")
    wscore = st.slider("Model score weight", 0.0, 1.0, 0.6, 0.05)
    wvalue = st.slider("Estimate value weight", 0.0, 1.0, 0.3, 0.05)
    wsla = st.slider("SLA urgency weight", 0.0, 1.0, 0.1, 0.05)
    baseline = st.number_input("Value baseline (for normalization)", value=5000.0)
    if st.button("Save weights"):
        st.session_state.weights = {"score_w": wscore, "value_w": wvalue, "sla_w": wsla, "value_baseline": baseline}
        st.success("Weights updated (in session)")


    st.markdown("---")
    st.subheader("Audit Trail")
    s = get_session()
    try:
        hist = s.query(LeadHistory).order_by(LeadHistory.timestamp.desc()).limit(200).all()
        if hist:
            hist_df = pd.DataFrame([{"lead_id":h.lead_id,"changed_by":h.changed_by,"field":h.field,"old":h.old_value,"new":h.new_value,"timestamp":h.timestamp} for h in hist])
            st.dataframe(hist_df)
        else:
            st.info("No audit entries yet.")
    finally:
        s.close()


# Exports page
def page_exports():
    require_role_access("exports")
    st.markdown("<div class='header'>üì§ Exports & Imports</div>", unsafe_allow_html=True)
    # üîí Plan Access Gate
    if not has_access("analytics_intelligence"):
        st.warning("üîí Analytics & Business Intelligence is not available on your current plan.")
        st.info("Upgrade your plan to unlock executive analytics, trends, and insights.")
        return

    st.markdown("<em>Export leads, import CSV/XLSX. Imported rows upsert by lead_id.</em>", unsafe_allow_html=True)
    df = leads_to_df(None, None)
    if not df.empty:
        towrite = io.BytesIO()
        df.to_excel(towrite, index=False, engine="openpyxl")
        towrite.seek(0)
        b64 = base64.b64encode(towrite.read()).decode()
        href = f"data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}"
        st.markdown(f'<a href="{href}" download="leads_export.xlsx">Download leads_export.xlsx</a>', unsafe_allow_html=True)
    uploaded = st.file_uploader("Upload leads (CSV/XLSX) for import/upsert", type=["csv","xlsx"])
    if uploaded:
        try:
            if uploaded.name.lower().endswith(".csv"):
                df_in = pd.read_csv(uploaded)
            else:
                df_in = pd.read_excel(uploaded)
            if "lead_id" not in df_in.columns:
                st.error("File must include a lead_id column")
            else:
                count = 0
                for _, r in df_in.iterrows():
                    try:
                        upsert_lead_record({
                            "lead_id": str(r["lead_id"]),
                            "created_at": pd.to_datetime(r.get("created_at")) if r.get("created_at") is not None else datetime.utcnow(),
                            "source": r.get("source"),
                            "contact_name": r.get("contact_name"),
                            "contact_phone": r.get("contact_phone"),
                            "contact_email": r.get("contact_email"),
                            "property_address": r.get("property_address"),
                            "damage_type": r.get("damage_type"),
                            "assigned_to": r.get("assigned_to"),
                            "notes": r.get("notes"),
                            "estimated_value": float(r.get("estimated_value") or 0.0),
                            "ad_cost": float(r.get("ad_cost") or 0.0),
                            "stage": r.get("stage") or "New",
                            "converted": bool(r.get("converted") or False)
                        }, actor="admin")
                        count += 1
                    except Exception:
                        continue
                st.success(f"Imported/Upserted {count} rows.")
        except Exception as e:
            st.error("Failed to import: " + str(e))


# ---------- BEGIN BLOCK F: FLASK API FOR LOCATION PINGS (optional but ready) ----------
try:
    from flask import Flask, request, jsonify
    import threading
    flask_app = Flask("recapture_pro_api")


    @flask_app.route("/api/ping_location", methods=["POST"])
    def api_ping_location():
        try:
            payload = request.get_json(force=True)
            tech = payload.get("tech_username") or payload.get("username")
            lat = payload.get("latitude") or payload.get("lat")
            lon = payload.get("longitude") or payload.get("lon")
            lead_id = payload.get("lead_id")
            accuracy = payload.get("accuracy")
            ts = payload.get("timestamp")
            ts_parsed = None
            if ts:
                try:
                    ts_parsed = datetime.fromisoformat(ts)
                except Exception:
                    ts_parsed = None
            if not tech or lat is None or lon is None:
                return jsonify({"error":"missing fields (tech_username, latitude, longitude)"}), 400
            pid = persist_location_ping(tech_username=str(tech), latitude=float(lat), longitude=float(lon), lead_id=lead_id, accuracy=accuracy, timestamp=ts_parsed)
            return jsonify({"ok": True, "ping_id": pid}), 200
        except Exception as e:
            return jsonify({"error": str(e)}), 500


    def run_flask():
        try:
            # choose port 5001 to avoid Streamlit port conflicts
            flask_app.run(host="0.0.0.0", port=5001, debug=False, use_reloader=False)
        except Exception:
            pass


    # start flask in background daemon thread (only if not already started)
    t = threading.Thread(target=run_flask, daemon=True)
    t.start()
except Exception:
    # if Flask isn't available (not installed) the API simply won't start ‚Äî harmless
    pass
# ---------- END BLOCK F ----------
demand = {}
season_score = 0.5

def add_time_windows(hist_df):
    """
    Adds rolling time windows (3, 6, 12 months) for seasonal comparison
    """
    if hist_df.empty:
        return {}

    df = hist_df.copy()
    df["date"] = pd.to_datetime(df["date"])
    df = df.sort_values("date")

    latest_date = df["date"].max()

    windows = {
        "3_months": df[df["date"] >= latest_date - pd.DateOffset(months=3)],
        "6_months": df[df["date"] >= latest_date - pd.DateOffset(months=6)],
        "12_months": df[df["date"] >= latest_date - pd.DateOffset(months=12)],
    }

    return windows

# -------------------------------------------------------------
# SEASONAL TRENDS PAGE ‚Äî SINGLE SOURCE OF TRUTH
# -------------------------------------------------------------
def page_seasonal_trends():
    require_role_access("business_intelligence")
    import numpy as np
    import pandas as pd
    import plotly.express as px
    import plotly.graph_objects as go
    import streamlit as st

    st.markdown("## üå¶Ô∏è Seasonal Trends & Weather-Based Damage Insights")
        # üîí Plan Access Gate
    if not has_access("analytics_intelligence"):
        st.warning("üîí Analytics & Business Intelligence is not available on your current plan.")
        st.info("Upgrade your plan to unlock executive analytics, trends, and insights.")
        return

    st.markdown(
        """
    <em>
    Analyze historical weather patterns, forecast damage risk, and receive strategic recommendations for the property damage industry only. Get:
    <br>
    ‚Ä¢ <strong>Seasonal & Historical Weather Analysis</strong> ‚Üí Identifies recurring weather-driven risk patterns that affect property damage volume<br>
    ‚Ä¢ <strong>Damage Risk Forecasting</strong> ‚Üí Predicts surge periods and exposure levels to enable proactive planning<br>
    ‚Ä¢ <strong>Weather-to-Business Impact Mapping</strong> ‚Üí Explains how weather trends influence claims, leads, and operational costs<br>
    ‚Ä¢ <strong>Strategic Recommendations Engine</strong> ‚Üí Provides time-based guidance for staffing, marketing, and operational decisions<br><br>
    </em>
        """,
        unsafe_allow_html=True
    )


    # =========================================================
    # 1. LOCATION SELECTION
    # =========================================================
    countries = get_all_countries()
    country = st.selectbox("Country", [c["name"] for c in countries])
    country_code = next(c["code"] for c in countries if c["name"] == country)

    city_query = st.text_input("City (e.g. Miami, London, Toronto)")
    matches = search_cities(country_code, city_query)

    if not matches:
        st.info("Start typing a city name.")
        return

    labels = [
        f"{m['name']}, {m['admin1']}" if m["admin1"] else m["name"]
        for m in matches
    ]
    selected = st.selectbox("Select city", labels)
    chosen = matches[labels.index(selected)]

    st.success(f"üìç {selected}, {country}")

    # =========================================================
    # 2. CONTROLS
    # =========================================================
    hist_range = st.selectbox(
        "Historical window",
        ["3 months", "6 months", "12 months"],
        index=1
    )

    forecast_range = st.selectbox(
        "Forecast horizon",
        ["3 months", "6 months", "12 months"]
    )

    months = {"3 months": 3, "6 months": 6, "12 months": 12}[hist_range]
    forecast_months = {"3 months": 3, "6 months": 6, "12 months": 12}[forecast_range]

    if not st.button("Generate Insights"):
        return

    # =========================================================
    # 3. DATA FETCH
    # =========================================================
    with st.spinner("Generating insights..."):
        hist_df = fetch_weather(chosen["lat"], chosen["lon"], months)
        forecast_days = forecast_months * 30
        forecast_df = fetch_forecast_weather(chosen["lat"], chosen["lon"], forecast_days)

    # =========================================================
    # 4. SAFETY CHECKS
    # =========================================================
    if hist_df.empty:
        st.error("Historical weather data unavailable for this location.")
        return

    if forecast_df.empty:
        st.warning(
            "‚ö†Ô∏è Forecast limited by API. Longer-range outlook inferred from historical patterns."
        )
        forecast_df = hist_df.copy()

    # =========================================================
    # 5. FEATURE ENGINEERING (SINGLE PASS)
    # =========================================================
    for df_ in [hist_df, forecast_df]:
        df_["humidity_pct"] = np.clip(60 + df_["rainfall_mm"] * 0.3, 30, 100)
        df_["storm_flag"] = (df_["rainfall_mm"] >= 20).astype(int)
        df_["water_damage_prob"] = np.clip(df_["rainfall_mm"] / 120, 0, 1)
        df_["mold_prob"] = np.clip(df_["humidity_pct"] / 100, 0, 1)
        df_["roof_storm_prob"] = df_["storm_flag"]
        df_["freeze_burst_prob"] = np.clip((df_["temperature_c"] < 1).astype(int), 0, 1)

    # =========================================================
    # 6. DEMAND DISTRIBUTION & SEASON SCORE
    # =========================================================
    demand = {
        "Water Damage": float(forecast_df["water_damage_prob"].mean()),
        "Mold Remediation": float(forecast_df["mold_prob"].mean()),
        "Storm / Roof": float(forecast_df["roof_storm_prob"].mean()),
        "Freeze / Pipe Burst": float(forecast_df["freeze_burst_prob"].mean()),
    }
    demand = {k: max(0.0, min(v, 1.0)) for k, v in demand.items()}
    season_score = round(np.mean(list(demand.values())), 2)

    # =========================================================
    # 7. TIME WINDOWS (3 / 6 / 12 MONTHS)
    # =========================================================
    windows = add_time_windows(hist_df)

    for i, (label, wdf) in enumerate(windows.items()):
        st.markdown(f"**Last {label.upper()}**")
        col1, col2 = st.columns(2)
        with col1:
            fig_temp = px.line(
                wdf, x="date", y="temperature_c", title=f"Average Temperature ({label})"
            )
            st.plotly_chart(fig_temp, use_container_width=True, key=f"temp_chart_{i}")
        with col2:
            fig_hum = px.line(
                wdf, x="date", y="humidity_pct", title=f"Average Humidity ({label})"
            )
            st.plotly_chart(fig_hum, use_container_width=True, key=f"hum_chart_{i}")

    # =========================================================
    # 8. SEASONAL TREND ANALYSIS ‚Äî HISTORY VS FORECAST
    # =========================================================
    trend_fig = go.Figure()
    trend_fig.add_trace(go.Scatter(
        x=hist_df["date"], y=hist_df["rainfall_mm"], name="Rainfall (Historical)", mode="lines"
    ))
    trend_fig.add_trace(go.Scatter(
        x=forecast_df["date"], y=forecast_df["rainfall_mm"], name="Rainfall (Forecast)", mode="lines", line=dict(dash="dash")
    ))
    trend_fig.add_trace(go.Scatter(
        x=hist_df["date"], y=hist_df["temperature_c"], name="Temperature (Historical)", mode="lines", yaxis="y2"
    ))
    trend_fig.add_trace(go.Scatter(
        x=forecast_df["date"], y=forecast_df["temperature_c"], name="Temperature (Forecast)", mode="lines", line=dict(dash="dash"), yaxis="y2"
    ))
    trend_fig.update_layout(
        xaxis_title="Date",
        yaxis=dict(title="Rainfall (mm)"),
        yaxis2=dict(title="Temperature (¬∞C)", overlaying="y", side="right"),
        legend=dict(orientation="h"),
        height=450
    )
    st.plotly_chart(trend_fig, use_container_width=True, key="trend_fig")

    # =========================================================
    # 9. DAMAGE RISK TRENDS
    # =========================================================
    risk_fig = go.Figure()
    for col, label_ in [
        ("water_damage_prob", "Water Damage"),
        ("mold_prob", "Mold"),
        ("roof_storm_prob", "Storm / Roof"),
        ("freeze_burst_prob", "Freeze / Pipe Burst"),
    ]:
        risk_fig.add_trace(go.Scatter(
            x=hist_df["date"], y=hist_df[col], name=f"{label_} (History)", mode="lines"
        ))
        risk_fig.add_trace(go.Scatter(
            x=forecast_df["date"], y=forecast_df[col], name=f"{label_} (Forecast)", mode="lines", line=dict(dash="dash")
        ))
    risk_fig.update_layout(yaxis=dict(range=[0,1]), legend=dict(orientation="h"), height=450)
    st.plotly_chart(risk_fig, use_container_width=True, key="risk_fig")

    # =========================================================
    # 10. WEATHER CHARTS ‚Äî HISTORICAL
    # =========================================================
    c1, c2 = st.columns(2)
    with c1:
        st.plotly_chart(
            px.line(hist_df, x="date", y="rainfall_mm", title="Historical Rainfall"),
            use_container_width=True,
            key="hist_rainfall"
        )
    with c2:
        st.plotly_chart(
            px.line(hist_df, x="date", y="temperature_c", title="Historical Temperature"),
            use_container_width=True,
            key="hist_temp"
        )

    # =========================================================
    # 11. EXECUTIVE SEASONAL INSIGHTS
    # =========================================================
    st.subheader("üß† Executive Seasonal Insights")
    insights = generate_seasonal_insights(leads_df, hist_df)
    if not insights:
        st.info("No significant seasonal signals detected for the selected period.")
    else:
        for i, insight in enumerate(insights):
            st.info(insight, icon="üí°")

    # =========================================================
    # 12. SUMMARY METRICS & NARRATIVE
    # =========================================================
    summary = {
        "avg_rain_hist": hist_df["rainfall_mm"].mean(),
        "avg_rain_forecast": forecast_df["rainfall_mm"].mean(),
        "avg_temp_hist": hist_df["temperature_c"].mean(),
        "avg_temp_forecast": forecast_df["temperature_c"].mean(),
        "avg_water_risk": forecast_df["water_damage_prob"].mean(),
        "avg_mold_risk": forecast_df["mold_prob"].mean(),
        "avg_storm_risk": forecast_df["roof_storm_prob"].mean(),
        "avg_freeze_risk": forecast_df["freeze_burst_prob"].mean(),
    }

    st.markdown("## üìù Narrative Analysis")
    notes = []
    if summary["avg_rain_forecast"] > summary["avg_rain_hist"] * 1.15:
        notes.append("üåßÔ∏è Rainfall is trending above seasonal norms ‚Äî water intrusion risk rising.")
    elif summary["avg_rain_forecast"] < summary["avg_rain_hist"] * 0.85:
        notes.append("üå§Ô∏è Rainfall below normal ‚Äî flood exposure reduced.")
    else:
        notes.append("üå¶Ô∏è Rainfall is seasonally stable.")

    if summary["avg_temp_forecast"] > summary["avg_temp_hist"] + 2:
        notes.append("üî• Warmer temperatures may elevate mold risk.")
    elif summary["avg_temp_forecast"] < summary["avg_temp_hist"] - 2:
        notes.append("‚ùÑÔ∏è Colder temperatures increase freeze/pipe burst risk.")
    else:
        notes.append("üå°Ô∏è Temperatures remain within seasonal norms.")

    for n in notes:
        st.info(n)

    # =========================================================
    # ‚úÖ 12a. EXECUTIVE NARRATIVE (INSERT HERE)
    # =========================================================
    st.markdown("## üß† Executive Summary")
    for line in data["executive_narrative"]:
        st.info(line)

    # =========================================================
    # 13. DAMAGE RISK OUTLOOK & RECOMMENDATIONS
    # =========================================================
    st.markdown("## üß† Damage Risk & Recommendations")
    risk_notes = []
    if summary["avg_water_risk"] > 0.55:
        risk_notes.append("üíß High likelihood of water damage events.")
    if summary["avg_mold_risk"] > 0.45:
        risk_notes.append("ü¶† Elevated mold growth risk.")
    if summary["avg_storm_risk"] > 0.4:
        risk_notes.append("üå™Ô∏è Storm-related roof damage risk.")
    if summary["avg_freeze_risk"] > 0.3:
        risk_notes.append("‚ùÑÔ∏è Freeze/pipe burst risk present.")
    if not risk_notes:
        risk_notes.append("üü¢ Overall weather-driven damage risk is low.")
    for r in risk_notes:
        st.warning(r)

    if season_score >= 0.6:
        st.error("üî• Peak Season ‚Äî increase staffing, pre-stage equipment, boost emergency marketing.")
    elif season_score >= 0.4:
        st.warning("‚ö†Ô∏è Elevated Activity ‚Äî flexible scheduling and monitor leads closely.")
    else:
        st.success("üü¢ Low / Normal Season ‚Äî focus on branding, SEO, and internal optimization.")

    # =========================================================
    # 14. EXPECTED LEADS & STAFFING
    # =========================================================
    BASE_MONTHLY_LEADS = 40
    expected_total_leads = int(BASE_MONTHLY_LEADS * (0.6 + season_score) * forecast_months)
    st.markdown("## üî¢ Expected Lead Volume")
    st.success(f"üìà ~{expected_total_leads} jobs over {forecast_months} months")
    techs = max(1, int(np.ceil(expected_total_leads / (18 * forecast_months))))
    st.metric("Recommended Technicians", techs)



# ---------- BEGIN BLOCK E: PAGE ‚Äì COMPETITOR INTELLIGENCE ----------
def page_competitor_intelligence():
    st.title("üèÜ Competitor Intelligence")

    # ===============================
    # üö® COMPETITIVE ALERTS (TOP)
    # ===============================
    st.subheader("üö® Competitive Alerts")

    s = get_session()
    alerts = (
        s.query(CompetitorAlert)
        .order_by(CompetitorAlert.created_at.desc())
        .limit(5)
        .all()
    )
    s.close()

    if not alerts:
        st.success("No competitive threats detected.")
    else:
        for i, a in enumerate(alerts):
            key = f"alert_{i}"
            if a.severity == "high":
                st.error(a.message, key=key)
            else:
                st.warning(a.message, key=key)

    st.divider()

    # ===============================
    # üîç COMPETITOR DISCOVERY
    # ===============================
    with st.expander("‚ûï Discover Competitors"):
        lat = st.number_input("Latitude", value=39.9612, key="comp_lat")
        lon = st.number_input("Longitude", value=-82.9988, key="comp_lon")
        keyword = st.text_input(
            "Search keyword",
            "water damage restoration",
            key="comp_keyword"
        )

        if st.button("Run Competitor Scan", key="run_comp_scan"):
            ingest_competitors_openstreetmap(lat, lon, keyword)
            st.success("Competitor scan completed.")

    # ===============================
    # üìä FETCH COMPETITORS
    # ===============================
    s = get_session()
    try:
        competitors = s.query(Competitor).all()
    finally:
        s.close()

    if not competitors:
        st.info("No competitors tracked yet.")
        return

    # ===============================
    # üìä BUILD COMPETITOR TABLE
    # ===============================
    rows = []
    hq_lat = st.session_state.get("hq_lat")
    hq_lon = st.session_state.get("hq_lon")

    for c in competitors:
        distance = 10
        if hq_lat and hq_lon and c.latitude and c.longitude:
            distance = haversine_km(hq_lat, hq_lon, c.latitude, c.longitude)

        score = calculate_competitor_score(
            c.rating or 0,
            c.total_reviews or 0,
            distance
        )

        rows.append({
            "Name": c.name,
            "Rating": c.rating,
            "Reviews": c.total_reviews,
            "Category": c.primary_category,
            "Distance (km)": round(distance, 2),
            "Strength Score": score,
            "Velocity (7d)": review_velocity(c.id, 7),
            "Velocity (30d)": review_velocity(c.id, 30),
        })

    df = pd.DataFrame(rows).sort_values(
        "Strength Score", ascending=False
    )

    st.subheader("Top Competitors")
    st.dataframe(df, use_container_width=True, key="competitor_table")

    # ===============================
    # üìä MARKET PRESSURE
    # ===============================
    def market_pressure_score(df):
        if df.empty:
            return 0
        return round(
            df["Velocity (7d)"].mean() * 0.4 +
            df["Strength Score"].mean() * 60,
            1
        )

    pressure = market_pressure_score(df)

    st.metric(
        "Market Pressure Score",
        pressure,
        delta="Rising" if pressure > 60 else "Stable",
        key="market_pressure"
    )

    # ===============================
    # üìâ SEO VISIBILITY GAP
    # ===============================
    st.subheader("üìâ SEO Visibility Gap")

    you_reviews = st.number_input("Your total reviews", value=120, key="you_reviews")
    you_rating = st.number_input("Your rating", value=4.6, key="you_rating")

    if not df.empty:
        gap = seo_visibility_gap(you_reviews, you_rating, df)

        if gap["pressure"] == "HIGH":
            st.error(
                f"Competitors average {gap['review_gap']} more reviews and "
                f"{gap['rating_gap']} higher rating. SEO pressure is HIGH.",
                key="seo_gap_high"
            )
        else:
            st.warning(
                "You are competitive, but review velocity must be maintained.",
                key="seo_gap_warn"
            )

    # ===============================
    # üß† EXECUTIVE COMPETITIVE SUMMARY
    # ===============================
    st.subheader("üß† Executive Competitive Summary")

    if not df.empty:
        top = df.iloc[0]
        st.markdown(f"""
**Market Overview**

The local restoration market is currently under **{gap['pressure']} competitive pressure**.

**Key Threat**
- {top['Name']} leads the market with {top['Reviews']} reviews and rapid growth velocity.

**Risk Outlook**
- Continued review acceleration from competitors could reduce inbound lead share.
- Immediate review acquisition and proximity-focused SEO are recommended.

**Recommended Actions**
1. Launch review campaigns immediately
2. Optimize GMB categories and services
3. Increase local landing page coverage
""", unsafe_allow_html=True)


# ---------- END BLOCK E ----------
# ----------------------
# WORDPRESS AUTH BRIDGE (GLOBAL)
# ----------------------

if "token" in st.query_params:
    wp_auth_bridge()
    st.stop()
# ----------------------
# NAVIGATION Side Bar Control
# ----------------------
with st.sidebar:
    st.header("ReCapture Pro")
    
    user = get_current_user()
    role = user.role if user else "Viewer"

    allowed_pages = ROLE_PERMISSIONS.get(role, set())

    PAGE_MAP = {
        "Overview": ("overview", page_overview),
        "Lead Capture": ("lead_capture", page_lead_capture),
        "Pipeline Board": ("pipeline", page_pipeline_board),
        "Analytics": ("analytics", page_analytics),
        "CPA & ROI": ("analytics", page_cpa_roi),
        "Tasks": ("tasks", page_tasks),
        "AI Recommendations": ("business_intelligence", page_ai_recommendations),
        "Seasonal Trends": ("business_intelligence", page_seasonal_trends),
        "Settings": ("settings", page_settings),
        "Exports": ("exports", page_exports),
    }

    visible_pages = {
        label: func
        for label, (key, func) in PAGE_MAP.items()
        if key in allowed_pages
    }

    choice = st.radio("Navigate", list(visible_pages.keys()))
    visible_pages[choice]()

    if get_current_user():
    if st.button("üö™ Logout"):
        st.session_state.pop("user_id", None)
        st.rerun()


# ---------- END BLOCK E ----------






# Footer
st.markdown("---")
st.markdown("<div class='small-muted'>ReCapture Pro. SQLite persistence. Integrated Field Tracking (upgrade) enabled.</div>", unsafe_allow_html=True)
